{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6db77d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle \n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "from src.utils.tree_utils import *\n",
    "from src.utils.utils import *\n",
    "from src.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e3fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './data/aliceysu'\n",
    "journalist = 'aliceysu'\n",
    "with open(os.path.join(out_dir, f'{journalist}_dict.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(out_dir, f'{journalist}_ids.pkl'), 'rb') as f:\n",
    "    map_id = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(out_dir, f'{journalist}_lan.pkl'), 'rb') as f:\n",
    "    map_lan = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(out_dir, f'{journalist}_type.pkl'), 'rb') as f:\n",
    "    map_type = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(out_dir, f'{journalist}_reply.pkl'), 'rb') as f:\n",
    "    map_reply = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(out_dir, f'{journalist}_edgeprob.pkl'), 'rb') as f:\n",
    "    edgeprob = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, journalist, classes, batch_size, collate):\n",
    "    ### Data (normalize input inter-event times, then padding to create dataloaders)\n",
    "    num_classes, num_sequences = classes, 0\n",
    "    seq_dataset = []\n",
    "    arr = []\n",
    "    dp = []\n",
    "    rel = []\n",
    "    \n",
    "    split = [64, 128]\n",
    "    val = 0\n",
    "    journal_sort = pd.read_csv((os.path.join(data_dir, f'{journalist}_context.csv')))\n",
    "    ids = list(set(journal_sort['conversation_id']))\n",
    "    id_pair = {}\n",
    "    id_conv = {}\n",
    "    for idx in ids:\n",
    "        id_pair[idx], id_conv[idx] = create_conversation_list(journal_sort[journal_sort['conversation_id']==idx], idx)\n",
    "    id_data, data, label = create_data(journal_sort, ids)\n",
    "    prob = pkl.load(open(os.path.join(data_dir, f'{journalist}_edgeprob.pkl'), 'rb'))\n",
    "    \n",
    "    with open(os.path.join(data_dir, f'{journalist}_global_path.txt'), \"r\") as f:\n",
    "        for line in tqdm(f, total=get_number_of_lines(f)):\n",
    "            dp.append(json.loads(line.strip()))\n",
    "\n",
    "    with open(os.path.join(data_dir, f'{journalist}_local_path.txt'), \"r\") as f:\n",
    "        for line in tqdm(f, total=get_number_of_lines(f)):\n",
    "            rel.append(json.loads(line.strip()))\n",
    "    \n",
    "    global_input = convert_global(dp, id_data)\n",
    "    local_data = convert_local(rel)\n",
    "    local_mat = generate_local_mat(local_data, id_data)\n",
    "    local_input = create_mat(local_mat, mat_type='concat')\n",
    "    logging.info(f'loaded split {journalist}...')\n",
    "    # data - dict: dim_process, devtest, args, train, dev, test, index (train/dev/test given as)\n",
    "    # data[split] - list dicts {'time_since_start': at, 'time_since_last_event': dt, 'type_event': mark} or\n",
    "    # data[split] - dict {'arrival_times', 'delta_times', 'marks'}\n",
    "    # data['dim_process'] = Number of accounts = 119,298\n",
    "    # num_sequences: number of conversations of a journalist\n",
    "    num_classes = classes\n",
    "    num_sequences = len(set(journal_sort['conversation_id']))\n",
    "    \n",
    "    X_train, X_dev, X_test = data[:split[0]], data[split[0]:split[1]], data[split[1]:]\n",
    "    prob_train, prob_dev, prob_test = prob[:split[0]], prob[split[0]:split[1]], prob[split[1]:]\n",
    "    global_train, global_dev, global_test = global_input[:split[0]], global_input[split[0]:split[1]], global_input[split[1]:]\n",
    "    local_train, local_dev, local_test = local_input[:split[0]], local_input[split[0]:split[1]], local_input[split[1]:]\n",
    "    label_train, label_dev, label_test = label[:split[0]], label[split[0]:split[1]], label[split[1]:]\n",
    "\n",
    "    d_train = TreeDataset(X_train, prob_train, global_train, local_train, label_train)\n",
    "    d_val = TreeDataset(X_dev, prob_dev, global_dev, local_dev, label_dev)  \n",
    "    d_test  = TreeDataset(X_test, prob_test, global_test, local_test, label_test)   \n",
    "\n",
    "    # for padding input sequences to maxlen of batch for running on gpu, and arranging them by length efficient\n",
    "    collate = collate  \n",
    "    dl_train = torch.utils.data.DataLoader(d_train, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "    dl_val = torch.utils.data.DataLoader(d_val, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "    dl_test = torch.utils.data.DataLoader(d_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "    return dl_train, dl_val, dl_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "0fba7ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 419/419 [00:00<00:00, 469.94it/s]\n",
      "100%|██████████| 419/419 [00:00<00:00, 2197.12it/s]\n",
      "/u/yian3/coordinated/src/utils/utils.py:96: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(result)\n"
     ]
    }
   ],
   "source": [
    "train, val, test = load_data(out_dir, journalist, 3, 8, collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "dc021ba8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-405-ce56ce5a40f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m  \u001b[0;31m# Number of transformer layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequenceClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Example input and mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-405-ce56ce5a40f2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, hidden_dim, num_classes, num_layers, dropout)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Transformer Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mencoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, device, dtype)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n\u001b[0;32m--> 297\u001b[0;31m                                             **factory_kwargs)\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Implementation of Feedforward model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embed_dim must be divisible by num_heads\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qkv_same_embed_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SequenceClassifier(nn.Module):\n",
    "    def __init__(self, original_input_dim, input_dim, hidden_dim, num_classes, num_layers, dropout=0.1):\n",
    "        super(SequenceClassifier, self).__init__()\n",
    "        self.original_input_dim = original_input_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Linear layer to adjust input dimension\n",
    "        self.input_projection = nn.Linear(self.original_input_dim, self.input_dim)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=self.input_dim, nhead=8, dim_feedforward=self.hidden_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=self.num_layers)\n",
    "\n",
    "        # Classification Head\n",
    "        self.classifier = nn.Linear(self.input_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Project input to the desired dimension\n",
    "        x = self.input_projection(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = x.permute(1, 0, 2)  # Reshape x to [seq_length, batch_size, input_dim]\n",
    "        if mask is not None:\n",
    "            mask = mask.permute(1, 0)  # Reshape mask to [seq_length, batch_size]\n",
    "            mask = mask.unsqueeze(1)  # Add dimension for heads\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "\n",
    "        # Classification for each element in the sequence\n",
    "        x = x.permute(1, 0, 2)  # Reshape back to [batch_size, seq_length, input_dim]\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Example usage\n",
    "input_dim = 11  # Feature dimension of each element in the sequence\n",
    "hidden_dim = 512  # Hidden dimension for transformer encoder\n",
    "num_classes = 3  # Number of classes\n",
    "num_layers = 4  # Number of transformer layers\n",
    "\n",
    "model = SequenceClassifier(input_dim, hidden_dim, num_classes, num_layers)\n",
    "\n",
    "# Example input and mask\n",
    "example_input = torch.randn(8, 10, 11)\n",
    "example_mask = torch.tensor([[1,1,1,0,0], [1,1,1,1,1]], dtype=torch.bool)\n",
    "logits = model(example_input, example_mask)  # Output of shape [8, 10, 3] (batch_size, seq_length, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "066b8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.randn(8, 10, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "99224f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 11])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input.permute(1, 0, 2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34dad735",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes, num_sequences = 0, 0\n",
    "seq_dataset = []\n",
    "arr = []\n",
    "    \n",
    "split = [64, 128]\n",
    "val = 0\n",
    "#data = pkl.load(open(os.path.join(out_dir, f'{journalist}_dict.pkl'), 'rb'))\n",
    "#logging.info(f'loaded split aliceysu...')\n",
    "num_classes = 3\n",
    "num_sequences = len(set(data['conversation_id']))\n",
    "journal = pd.DataFrame.from_dict(data)\n",
    "#journal_sort = journal.sort_values(by=['created_at'])\n",
    "#journal_batch = journal_sort[[\"type\", \"possibly_sensitive\", \"lang\", \"reply_settings\",\n",
    "#                              \"retweet_count\", \"reply_count\", \"like_count\", \"quote_count\", \"impression_count\",\n",
    "#                              \"mentions\", \"urls\", \"labels\"]]\n",
    "journal_sort = pd.read_csv(os.path.join(out_dir, f'{journalist}_context.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c42f16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(set(journal_sort['conversation_id']))\n",
    "id_pair = {}\n",
    "id_conv = {}\n",
    "for idx in ids:\n",
    "    id_pair[idx], id_conv[idx] = create_conversation_list(journal_sort[journal_sort['conversation_id']==idx], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d80c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, f'{journalist}_global_path.txt')) as f:\n",
    "    global_path = f.readlines()\n",
    "    \n",
    "with open(os.path.join(out_dir, f'{journalist}_local_path.txt')) as f:\n",
    "    local_path = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_path(g_list):\n",
    "    g_dict = {}\n",
    "    for item in g_list:\n",
    "        temp = ast.literal_eval(item)\n",
    "        for item in temp:\n",
    "            k = list(item.keys())[0]\n",
    "            g_dict[k] = item[k][k]\n",
    "    return g_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7d71ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dict = convert_path(global_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ea434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#journal_sort['global'] = [global_dict.get(str(id), global_dict[id]) for id in journal_sort['tweet_id']]\n",
    "#journal_sort.to_csv(os.path.join(out_dir, f'{journalist}_context.csv'))\n",
    "global_paths = [] # input\n",
    "id_clean = {}\n",
    "for i in ids:\n",
    "    temp = []\n",
    "    id_clean[i] = []\n",
    "    for j, item in enumerate(id_conv[i]):\n",
    "        if str(item) not in global_dict.keys():\n",
    "            continue\n",
    "        temp.append(global_dict[str(item)])\n",
    "        id_clean[i].append(item)\n",
    "    global_paths.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03fe4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(out_dir, f'{journalist}_global_path.txt'), \"w\") as fout:\n",
    "#     num_dps = 0\n",
    "#     for k in id_pair.keys():\n",
    "#         tree_root = build_tree(id_pair[k])\n",
    "        \n",
    "#         tree_root.create_global_relation()\n",
    "#         node_list = tree_root.dfs()\n",
    "\n",
    "#         root_paths = TreeNode.extract_data(node_list,f=lambda node: clamp_and_slice_ids(\n",
    "#                 node.global_relation, max_width=-1, max_depth=-1))\n",
    "#         asts = separate_dps(root_paths, n_ctx)\n",
    "\n",
    "#         \"\"\"for lr, extended in asts:\n",
    "#             if extended != 0:\n",
    "#                 break\n",
    "#             if len(lr) - extended > 1:\n",
    "#                 \"\"\"\n",
    "#         json.dump(root_paths, fp=fout)  # each line is the json of a list [dict,dict,...]\n",
    "#         num_dps += 1\n",
    "#         fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "91e06829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_dps = 0\n",
    "# with open(os.path.join(out_dir, f'{journalist}_local_path.txt'), \"w\") as fout:\n",
    "#     for k in id_pair.keys():\n",
    "#         tree_root = build_tree(id_pair[k])\n",
    "        \n",
    "#         tree_root.create_local_relation()\n",
    "#         node_list = tree_root.dfs()\n",
    "\n",
    "#         local_relation = TreeNode.extract_data(node_list,f=lambda node: clamp_and_slice_ids(\n",
    "#                 node.local_relation, max_width=-1, max_depth=-1))\n",
    "#         rel = separate_dps(local_relation, n_ctx)\n",
    "\n",
    "#         \"\"\"for lr, extended in rel:\n",
    "#             if extended != 0:\n",
    "#                 break\n",
    "#             if len(lr) - extended > 1:\"\"\"\n",
    "#         json.dump(local_relation, fp=fout)  # each line is the json of a list [dict,dict,...]\n",
    "#         num_dps += 1\n",
    "#         fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67ef66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5582b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fp = './result'\n",
    "journalist = 'aliceysu'\n",
    "n_ctx = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11bb1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = []\n",
    "target_data = []\n",
    "conv_data = []\n",
    "ref_data = []\n",
    "id_data = []\n",
    "for idx in ids:\n",
    "    convs = journal_sort[journal_sort['conversation_id'] == idx]\n",
    "    convs_batch = convs[[\"type\", \"possibly_sensitive\", \"lang\", \"reply_settings\",\n",
    "                     \"retweet_count\", \"reply_count\", \"like_count\", \"quote_count\", \"impression_count\",\n",
    "                     \"mentions\", \"urls\"]]\n",
    "    conv_data.append(list(convs['conversation_id']))\n",
    "    ref_data.append(list(convs['reference_id']))\n",
    "    id_data.append(list(convs['tweet_id']))\n",
    "    batch_data.append(convs_batch.values.tolist())\n",
    "    target_data.append(list(convs['labels']))\n",
    "    \n",
    "label_data = target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3890f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = []\n",
    "for i, idx in enumerate(ids):\n",
    "    tree_root = build_tree(id_pair[idx])\n",
    "    node_info = get_node_info(tree_root)\n",
    "    temp_list = []\n",
    "    #temp_list.append([node_info[idx]['level'], node_info[idx]['number_of_siblings'], node_info[idx]['sibling_order']])\n",
    "    for item in id_data[i]:\n",
    "        \"\"\"if item not in node_info.keys():\n",
    "            continue\"\"\"\n",
    "        temp_list.append([node_info[item]['level'], \n",
    "                    node_info[item]['number_of_siblings'], \n",
    "                    node_info[item]['sibling_order']])\n",
    "    position_data.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13fbd99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, X_test = batch_data[:split[0]], batch_data[split[0]:split[1]], batch_data[split[1]:]\n",
    "pos_train, pos_dev, pos_test = position_data[:split[0]], position_data[split[0]:split[1]], position_data[split[1]:]\n",
    "id_train, id_dev, id_test = id_data[:split[0]], id_data[split[0]:split[1]], id_data[split[1]:]\n",
    "label_train, label_dev, label_test = label_data[:split[0]], label_data[split[0]:split[1]], label_data[split[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7f9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249f4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4b8e746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 419/419 [00:01<00:00, 395.10it/s]\n",
      "100%|██████████| 419/419 [00:00<00:00, 1766.06it/s]\n"
     ]
    }
   ],
   "source": [
    "dp = []\n",
    "with open(os.path.join(out_fp, f'{journalist}_global_path.txt'), \"r\") as f:\n",
    "    for line in tqdm(f, total=get_number_of_lines(f)):\n",
    "        dp.append(json.loads(line.strip()))\n",
    "        \n",
    "rel = []\n",
    "with open(os.path.join(out_fp, f'{journalist}_local_path.txt'), \"r\") as f:\n",
    "    for line in tqdm(f, total=get_number_of_lines(f)):\n",
    "        rel.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9eb82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = convert_global(dp, id_data)\n",
    "local = convert_local(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ebac96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, f'{journalist}_global_path.pkl'),'wb') as f:\n",
    "    pickle.dump(roots, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e15ae513",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'4112'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-7e3dfddf3ec5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnew_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mroots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-236-7e3dfddf3ec5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnew_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mroots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '4112'"
     ]
    }
   ],
   "source": [
    "## test\n",
    "max_len = max(len(dp[0]) for dp in batch_data[:8])\n",
    "max_depth = 12\n",
    "max_width = 16\n",
    "seqs = (id_data[:8], batch_data[:8], roots[:8])\n",
    "r = []\n",
    "for i in range(8):\n",
    "    new_root = [roots[i][str(x)] for x in id_data[i]][:] + [[] for _ in range(max_len - len(id_data[i]))]\n",
    "    r.append(new_root)\n",
    "    \n",
    "positions = generate_positions(r[1], max_width, max_depth)\n",
    "position_seqs = []\n",
    "positions = generate_positions(r[1], max_width=max_width, max_depth=max_depth)\n",
    "position_seqs.append(positions.unsqueeze(0))\n",
    "\n",
    "position_seqs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "710357f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/yian3/.conda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "local_mat = generate_local_mat(local, id_data)\n",
    "local_input = create_mat(local_mat, mat_type='concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b27e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc557f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17de4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907499b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb4d26fd",
   "metadata": {},
   "source": [
    "## test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5534df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(journal_sort, ids):\n",
    "    batch_data = []\n",
    "    target_data = []\n",
    "    conv_data = []\n",
    "    ref_data = []\n",
    "    id_data = []\n",
    "    for idx in ids:\n",
    "        convs = journal_sort[journal_sort['conversation_id'] == idx]\n",
    "        convs_batch = convs[['type', 'possibly_sensitive', 'lang', 'reply_settings', \n",
    "                               'retweet_count', 'reply_count', 'like_count', 'quote_count',\n",
    "                                'impression_count', 'mentions', 'urls']]\n",
    "        #conv_data.append(list(convs['conversation_id']))\n",
    "        conv_data.append(convs_batch.to_numpy().tolist())\n",
    "        ref_data.append(list(convs['reference_id']))\n",
    "        id_data.append(list(convs['tweet_id']))\n",
    "        batch_data.append(convs_batch.values.tolist())\n",
    "        target_data.append(list(convs['labels']))\n",
    "    \n",
    "    label_data = target_data\n",
    "    return id_data, conv_data, label_data\n",
    "\n",
    "def load_data(data_dir, journalist, classes, batch_size, collate):\n",
    "    ### Data (normalize input inter-event times, then padding to create dataloaders)\n",
    "    num_classes, num_sequences = 0, 0\n",
    "    seq_dataset = []\n",
    "    arr = []\n",
    "    dp = []\n",
    "    rel = []\n",
    "    \n",
    "    split = [64, 128]\n",
    "    val = 0\n",
    "    journal_sort = pd.read_csv((os.path.join(data_dir, f'{journalist}_context.csv')))\n",
    "    ids = list(set(journal_sort['conversation_id']))\n",
    "    id_pair = {}\n",
    "    id_conv = {}\n",
    "    for idx in ids:\n",
    "        id_pair[idx], id_conv[idx] = create_conversation_list(journal_sort[journal_sort['conversation_id']==idx], idx)\n",
    "    id_data, data, label = create_data(journal_sort, ids)\n",
    "    prob = pkl.load(open(os.path.join(data_dir, f'{journalist}_edgeprob.pkl'), 'rb'))\n",
    "    \n",
    "    with open(os.path.join(data_dir, f'{journalist}_global_path.txt'), \"r\") as f:\n",
    "        for line in tqdm(f, total=get_number_of_lines(f)):\n",
    "            dp.append(json.loads(line.strip()))\n",
    "\n",
    "    with open(os.path.join(data_dir, f'{journalist}_local_path.txt'), \"r\") as f:\n",
    "        for line in tqdm(f, total=get_number_of_lines(f)):\n",
    "            rel.append(json.loads(line.strip()))\n",
    "    \n",
    "    global_input = convert_global(dp, id_data)\n",
    "    local_data = convert_local(rel)\n",
    "    local_mat = generate_local_mat(local_data, id_data)\n",
    "    local_input = create_mat(local_mat, mat_type='concat')\n",
    "    logging.info(f'loaded split {journalist}...')\n",
    "    # data - dict: dim_process, devtest, args, train, dev, test, index (train/dev/test given as)\n",
    "    # data[split] - list dicts {'time_since_start': at, 'time_since_last_event': dt, 'type_event': mark} or\n",
    "    # data[split] - dict {'arrival_times', 'delta_times', 'marks'}\n",
    "    # data['dim_process'] = Number of accounts = 119,298\n",
    "    # num_sequences: number of conversations of a journalist\n",
    "    num_classes = classes\n",
    "    #num_sequences += len(data[split]['arrival_times'])\n",
    "    num_sequences = len(set(journal_sort['conversation_id']))\n",
    "    \n",
    "    X_train, X_dev, X_test = data[:split[0]], data[split[0]:split[1]], data[split[1]:]\n",
    "    prob_train, prob_dev, prob_test = prob[:split[0]], prob[split[0]:split[1]], prob[split[1]:]\n",
    "    global_train, global_dev, global_test = global_input[:split[0]], global_input[split[0]:split[1]], global_input[split[1]:]\n",
    "    local_train, local_dev, local_test = local_input[:split[0]], local_input[split[0]:split[1]], local_input[split[1]:]\n",
    "    label_train, label_dev, label_test = label[:split[0]], label[split[0]:split[1]], label[split[1]:]\n",
    "\n",
    "    d_train = TreeDataset(X_train, prob_train, global_train, local_train, label_train)\n",
    "    d_val = TreeDataset(X_dev, prob_dev, global_dev, local_dev, label_dev)  \n",
    "    d_test  = TreeDataset(X_test, prob_test, global_test, local_test, label_test)   \n",
    "\n",
    "    # for padding input sequences to maxlen of batch for running on gpu, and arranging them by length efficient\n",
    "    collate = collate  \n",
    "    dl_train = torch.utils.data.DataLoader(d_train, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "    dl_val = torch.utils.data.DataLoader(d_val, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "    dl_test = torch.utils.data.DataLoader(d_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "    return dl_train, dl_val, dl_test\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNodes:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.children = []  # List of TreeNode objects\n",
    "        self.level = 0  # Level of the node in the tree\n",
    "        self.sibling_order = 0  # Order among siblings\n",
    "        self.parent = None  # Parent of the node\n",
    "        self.local_relation = dict()\n",
    "        self.global_relation = dict()\n",
    "\n",
    "    def add_child(self, child_node):\n",
    "        child_node.parent = self\n",
    "        child_node.level = self.level + 1 if self.level is not None else 0\n",
    "        child_node.sibling_order = len(self.children)\n",
    "        self.children.append(child_node)\n",
    "\n",
    "    def num_siblings(self):\n",
    "        return len(self.parent.children)-1 if self.parent else 0\n",
    "    \n",
    "    def extract_data(node_list, only_leaf=False, f=lambda node: node.data):\n",
    "        ret = []\n",
    "        #print(\"?\")\n",
    "        for node in node_list:\n",
    "            if not (only_leaf and node.node_type == \"type\"):\n",
    "                ret.append({node.name: f(node)})\n",
    "        return ret\n",
    "\n",
    "    def create_local_relation(self):\n",
    "\n",
    "        def _dfs(node):\n",
    "            for child in node.children:\n",
    "                node_child_rel = [child.level, child.num_siblings(), child.sibling_order]\n",
    "                node_father_rel = [node.level, node.num_siblings(), node.sibling_order]\n",
    "                #node_father_rel = child.parent\n",
    "                node.local_relation[child.name] = [node_child_rel, node_father_rel, 0]\n",
    "                child.local_relation[node.name] = [node_child_rel, node_father_rel, 1]\n",
    "                _dfs(child)\n",
    "\n",
    "        _dfs(self)\n",
    "\n",
    "    def create_global_relation(self):\n",
    "        def g_dfs(node):\n",
    "            node_rel = [node.level, node.num_siblings(), node.sibling_order]\n",
    "            if not node.parent:\n",
    "                node.global_relation[node.name] = [node_rel]\n",
    "            else: \n",
    "                if node.parent.name not in node.parent.global_relation.keys():\n",
    "                    node.global_relation[node.name] = node.parent.parent.global_relation[node.parent.parent.name] + [node_rel]\n",
    "                else:\n",
    "                    node.global_relation[node.name] = node.parent.global_relation[node.parent.name] + [node_rel]\n",
    "            for child in node.children:\n",
    "                g_dfs(child)\n",
    "\n",
    "        g_dfs(self)\n",
    " \n",
    "\n",
    "    def dfs(self):\n",
    "        ret = []\n",
    "\n",
    "        def _dfs(node, ret):\n",
    "           #ret : List\n",
    "            ret.append(node)\n",
    "            for child in node.children:\n",
    "                _dfs(child, ret)\n",
    "\n",
    "        _dfs(self, ret)\n",
    "        return ret\n",
    "    \n",
    "def build_tree(conversations):\n",
    "    nodes = {}\n",
    "    root = 0\n",
    "\n",
    "    for parent, child in conversations:\n",
    "        if parent not in nodes:\n",
    "            nodes[parent] = TreeNodes(parent)\n",
    "        if child not in nodes:\n",
    "            nodes[child] = TreeNodes(child)\n",
    "\n",
    "        nodes[parent].add_child(nodes[child])\n",
    "\n",
    "        if not root:\n",
    "            root = nodes[parent]\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "def separate_dps(ast, max_len):\n",
    "    \"\"\"\n",
    "    Handles training / evaluation on long ASTs by splitting\n",
    "    them into smaller ASTs of length max_len, with a sliding\n",
    "    window of max_len / 2.\n",
    "\n",
    "    Example: for an AST ast with length 1700, and max_len = 1000,\n",
    "    the output will be:\n",
    "    [[ast[0:1000], 0], [ast[500:1500], 1000], [ast[700:1700], 1500]]\n",
    "\n",
    "    Input:\n",
    "        ast : List[Dictionary]\n",
    "            List of nodes in pre-order traversal.\n",
    "        max_len : int\n",
    "\n",
    "    Output:\n",
    "        aug_asts : List[List[List, int]]\n",
    "            List of (ast, beginning idx of unseen nodes)\n",
    "    \"\"\"\n",
    "    half_len = int(max_len / 2)\n",
    "    if len(ast) <= max_len:\n",
    "        return [[ast, 0]]\n",
    "\n",
    "    aug_asts = [[ast[:max_len], 0]]\n",
    "    i = half_len\n",
    "    while i < len(ast) - max_len:\n",
    "        aug_asts.append([ast[i: i + max_len], half_len])\n",
    "        i += half_len\n",
    "    idx = max_len - (len(ast) - (i + half_len))\n",
    "    aug_asts.append([ast[-max_len:], idx])\n",
    "\n",
    "    return aug_asts\n",
    "\n",
    "\n",
    "def separate_lrs(lrs, max_len):\n",
    "    def reformat(lrs, left):  # [left,right)\n",
    "        new_lrs = []\n",
    "        for idx, lr in enumerate(lrs):\n",
    "            # lr -> dict: {idx:[],idx:[]}\n",
    "            temp_lr = dict()\n",
    "            for key, val in lr.items():\n",
    "                if left <= key < left + max_len:\n",
    "                    temp_lr[key - left] = val\n",
    "            new_lrs.append(temp_lr)\n",
    "        return new_lrs\n",
    "\n",
    "    half_len = int(max_len / 2)\n",
    "    if len(lrs) <= max_len:\n",
    "        return [[reformat(lrs, 0), 0]]\n",
    "\n",
    "    aug_asts = [[reformat(lrs[:max_len], 0), 0]]\n",
    "    i = half_len\n",
    "    while i < len(lrs) - max_len:\n",
    "        aug_asts.append([reformat(lrs[i: i + max_len], i), half_len])\n",
    "        i += half_len\n",
    "    idx = max_len - (len(lrs) - (i + half_len))\n",
    "    aug_asts.append([reformat(lrs[len(lrs) - max_len:], len(lrs) - max_len), idx])\n",
    "    return aug_asts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9738300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_global(root_paths, id_data):\n",
    "    roots = []\n",
    "    global_new = []\n",
    "    for i in range(len(root_paths)):\n",
    "        new_dict = {}\n",
    "        for item in root_paths[i]:\n",
    "            name = list(item.keys())[0]\n",
    "            new_dict[name] = np.array(list(list(item.values())[0].values())).squeeze().tolist()\n",
    "        roots.append(new_dict)\n",
    "    for i in range(len(id_data)):\n",
    "        global_new.append([roots[i][str(k)] for k in id_data[i]])\n",
    "    return global_new\n",
    "\n",
    "\n",
    "def convert_local(local_rel):\n",
    "    rel = []\n",
    "    for i in range(len(local_rel)):\n",
    "        new_dict = {}\n",
    "        for item in local_rel[i]:\n",
    "            name = list(item.keys())[0]\n",
    "            new_dict[name] = item[name]\n",
    "        rel.append(new_dict)\n",
    "    return rel\n",
    "\n",
    "def indexing(ls):\n",
    "    dic = {}\n",
    "    for i in range(len(ls)):\n",
    "        dic[ls[i]] = i\n",
    "        i += 1\n",
    "    return dic\n",
    "\n",
    "def generate_local_mat(local, idx):\n",
    "    mat = []\n",
    "    for ids, item in enumerate(local):\n",
    "        #print(ids)\n",
    "        temp = []\n",
    "        ind = indexing(idx[ids])\n",
    "        for i in idx[ids]:\n",
    "            if str(i) not in list(local[ids].keys()):\n",
    "                continue\n",
    "            for k in list(local[ids][str(i)].keys()):\n",
    "                if k == list(local[ids].keys())[0]:\n",
    "                    temp_l = local[ids][str(i)][k]\n",
    "                    temp_ind = ind[i]\n",
    "                    temp.append([temp_ind, temp_ind, temp_l[temp_l[2]]])\n",
    "                elif int(k) not in idx[ids]:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_l = local[ids][str(i)][k]\n",
    "                    temp_ind1 = ind[i]\n",
    "                    temp_ind2 = ind[int(k)]\n",
    "                    temp.append([temp_ind1, temp_ind2, temp_l[temp_l[2]]])\n",
    "        if not temp:\n",
    "            for i in idx[ids]:\n",
    "                temp_ind = ind[i]\n",
    "                temp.append([temp_ind, temp_ind, [0, 0, 0]])\n",
    "        mat.append(temp)\n",
    "    return mat\n",
    "def create_mat(local_mat, mat_type):\n",
    "    result = []\n",
    "    for ind, item in enumerate(local_mat):\n",
    "        max_row = max(i[0] for i in item)+1\n",
    "        max_col = max(i[1] for i in item)+1\n",
    "        if mat_type == 'sum':\n",
    "            row = np.array(item)[:,0]\n",
    "            col = np.array(item)[:,1]\n",
    "\n",
    "            # taking data \n",
    "            data = np.array([sum(np.array(i)[2]) for i in item])\n",
    "\n",
    "            # creating sparse matrix \n",
    "            sparseMatrix = csr_matrix((data, (row, col)), shape = (dim, dim)).toarray() \n",
    "            result.append(sparseMatrix)\n",
    "        else:\n",
    "            matrix = np.zeros((max_row, max_col, 3), dtype=float)\n",
    "            for x in item:\n",
    "                row, col, value = x\n",
    "                matrix[row, col] = [i + 0.05 for i in value]\n",
    "            result.append(matrix)\n",
    "    return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95dd877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_dim=None, pad_token=0):\n",
    "    # Determine the maximum sequence length\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    if max_dim is not None and max_length > max_dim:\n",
    "        max_length = max_dim\n",
    "\n",
    "    # Pad each sequence to the maximum length\n",
    "    padded_sequences = np.array([np.pad(seq, ((0, max_length - len(seq)), (0, 0)), \n",
    "                                        mode='constant', constant_values=pad_token) \n",
    "                                 for seq in sequences])\n",
    "\n",
    "    # Create attention masks\n",
    "    attention_masks = np.array([[1 if token.any() else 0 for token in seq] \n",
    "                                for seq in padded_sequences])\n",
    "    \n",
    "    return padded_sequences, attention_masks\n",
    "\n",
    "def pad_labels(labels, max_dim, pad_token=0):\n",
    "    \"\"\"Pad the label sequence to the maximum length.\"\"\"\n",
    "    max_length = max(len(seq) for seq in labels)\n",
    "    if max_dim is not None and max_length > max_dim:\n",
    "        max_length = max_dim\n",
    "        \n",
    "    return np.array([np.pad(seq, (0, max_length - len(seq)), mode='constant', constant_values=pad_token) \n",
    "                                 for seq in labels])\n",
    "\n",
    "def pad_matrix(path, max_dim=None, pad_token=0):\n",
    "    \"\"\"Pad a 2D matrix to the specified max_length.\"\"\"\n",
    "    max_length = max(matrix.shape[0] for matrix in path)\n",
    "    if max_dim is not None and max_length > max_dim:\n",
    "        max_length = max_dim\n",
    "        \n",
    "    padded_matrices = []\n",
    "    for matrix in path:\n",
    "        truncated_matrix = matrix[:max_length, :max_length, :]\n",
    "        padding = ((0, max(0, max_length - truncated_matrix.shape[0])), \n",
    "                   (0, max(0, max_length - truncated_matrix.shape[1])), \n",
    "                   (0, 0))\n",
    "        adjusted_matrix = np.pad(truncated_matrix, pad_width=padding, mode='constant', constant_values=pad_token)\n",
    "        padded_matrices.append(adjusted_matrix)\n",
    "\n",
    "    return padded_matrices\n",
    "\n",
    "def summ(paths):\n",
    "    return [[list(map(sum, zip(*sub))) for sub in outer] for outer in paths]\n",
    "\n",
    "\n",
    "class Batch():\n",
    "    def __init__(self, data, labels, prob, global_path, local_path, masks):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.prob = prob\n",
    "        self.global_path = global_path\n",
    "        self.local_path = local_path\n",
    "        self.masks = masks\n",
    "\n",
    "def collate(batch):\n",
    "    batch_li = [list(item) for item in batch]\n",
    "    data_temp = [row[0] for row in batch_li]\n",
    "    labels_temp = [torch.Tensor(row[1]) for row in batch_li]\n",
    "    prob_temp = [torch.Tensor(row[2]) for row in batch_li]\n",
    "    global_path_temp = [row[3] for row in batch_li]\n",
    "    local_path_temp = [row[4] for row in batch_li]\n",
    "    \n",
    "\n",
    "    padded_data, masks = pad_sequences(data_temp, max_dim=2000, pad_token=0)\n",
    "    #padded_labels = pad_labels(labels_temp, max_dim=2000, pad_token=0)\n",
    "    #padded_prob, _ = pad_sequences(prob_temp, max_dim=2000, pad_token=0)\n",
    "    padded_global, _ = pad_sequences(summ(global_path_temp), max_dim=2000, pad_token=0)\n",
    "    padded_local = pad_matrix(local_path_temp, max_dim=2000, pad_token=0)\n",
    "\n",
    "    data= torch.tensor(padded_data).to(torch.int64)\n",
    "    #labels = torch.tensor(padded_labels).to(torch.int64)\n",
    "    #prob = torch.tensor(padded_prob).to(torch.int64)\n",
    "    global_path = torch.tensor(padded_global).to(torch.int64)\n",
    "    local_path = torch.tensor(padded_local).to(torch.int64)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels_temp, batch_first=True)\n",
    "    prob = torch.nn.utils.rnn.pad_sequence(prob_temp, batch_first=True)\n",
    "    #global_path = torch.nn.utils.rnn.pad_sequence(global_path_temp, batch_first=True)\n",
    "    #local_path = torch.nn.utils.rnn.pad_sequence(local_path_temp, batch_first=True)\n",
    "    #print(masks)\n",
    "    \n",
    "    #out_tweet_type = torch.nn.utils.rnn.pad_sequence(out_tweet_types, batch_first=True)\n",
    "    #print(\"start\")\n",
    "    return Batch(data, labels, prob, global_path, local_path, torch.tensor(masks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.children = []  # List of TreeNode objects\n",
    "        self.level = 0  # Level of the node in the tree\n",
    "        self.sibling_order = 0  # Order among siblings\n",
    "        self.parent = None  # Parent of the node\n",
    "        self.local_relation = dict()\n",
    "        self.global_relation = dict()\n",
    "        \n",
    "\n",
    "    def add_child(self, child_node):\n",
    "        child_node.parent = self\n",
    "        child_node.level = self.level + 1 if self.level is not None else 0\n",
    "        child_node.sibling_order = len(self.children)\n",
    "        self.children.append(child_node)\n",
    "\n",
    "    def num_siblings(self):\n",
    "        return len(self.parent.children)-1 if self.parent else 0\n",
    "    \n",
    "    def extract_data(node_list, only_leaf=False, f=lambda node: node.data):\n",
    "        ret = []\n",
    "        for node in node_list:\n",
    "            if not (only_leaf and node.node_type == \"type\"):\n",
    "                ret.append(f(node))\n",
    "        return ret\n",
    "\n",
    "    def create_local_relation(self):\n",
    "        def _dfs(node):\n",
    "            for child in node.children:\n",
    "                node_child_rel = [child.level, child.num_siblings(), child.sibling_order]\n",
    "                node_father_rel = [node.level, node.num_siblings(), node.sibling_order]\n",
    "                \n",
    "                node.local_relation[child.name] = [node_child_rel, node_father_rel, 0]\n",
    "                child.local_relation[node.name] = [node_child_rel, node_father_rel, 1]\n",
    "                _dfs(child)\n",
    "\n",
    "        _dfs(self)\n",
    "    \n",
    "    def create_global_relation(self):\n",
    "        def g_dfs(node):\n",
    "            node_rel = [node.level, node.num_siblings(), node.sibling_order]\n",
    "            if not node.parent:\n",
    "                node.global_relation[node.name] = [node_rel]\n",
    "            else: \n",
    "                if node.parent.name not in node.parent.global_relation.keys():\n",
    "                    node.global_relation[node.name] = node.parent.parent.global_relation[node.parent.parent.name] + [node_rel]\n",
    "                else:\n",
    "                    node.global_relation[node.name] = node.parent.global_relation[node.parent.name] + [node_rel]\n",
    "            for child in node.children:\n",
    "                g_dfs(child)\n",
    "\n",
    "        g_dfs(self)\n",
    "        #return \n",
    "\n",
    "    def dfs(self):\n",
    "        ret = []\n",
    "\n",
    "        def _dfs(node, ret):\n",
    "           #ret : List\n",
    "            ret.append(node)\n",
    "            for child in node.children:\n",
    "                _dfs(child, ret)\n",
    "\n",
    "        _dfs(self, ret)\n",
    "        return ret\n",
    "    \n",
    "def build_tree(conversations):\n",
    "    nodes = {}\n",
    "    root = 0\n",
    "\n",
    "    for parent, child in conversations:\n",
    "        if parent not in nodes:\n",
    "            nodes[parent] = TreeNode(parent)\n",
    "        if child not in nodes:\n",
    "            nodes[child] = TreeNode(child)\n",
    "        nodes[parent].add_child(nodes[child])\n",
    "\n",
    "        if not root:\n",
    "            root = nodes[parent]\n",
    "\n",
    "    return root\n",
    "\n",
    "def get_node_info(tree_root):\n",
    "    node_info = {}\n",
    "\n",
    "    def traverse(node):\n",
    "        node_info[node.name] = {\n",
    "            'level': node.level,\n",
    "            'number_of_siblings': node.num_siblings(),\n",
    "            'sibling_order': node.sibling_order,\n",
    "        }\n",
    "        for child in node.children:\n",
    "            traverse(child)\n",
    "\n",
    "    traverse(tree_root)\n",
    "    return node_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d6bc5",
   "metadata": {},
   "source": [
    "## Mukhil's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca61587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aliceysu\n",
    "# bainjal\n",
    "users = []\n",
    "journalist = 'aliceysu'\n",
    "user_id = '24709718'\n",
    "data_dir = '../desktop/'\n",
    "path = os.path.join(data_dir, f'tweets_in_{journalist}_started_convs.json')\n",
    "# '../desktop/tweets_in_aliceysu_started_convs.json'\n",
    "with open(path) as f:\n",
    "    for line in f:\n",
    "        #print(line)\n",
    "        users.append(json.loads(line))\n",
    "    #users = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c40f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n"
     ]
    }
   ],
   "source": [
    "data = users[0]\n",
    "user_ids = set()\n",
    "for i in range(len(data)):  \n",
    "    user_ids.add(data[i]['conversation_id'])\n",
    "print(len(user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2f2ce8e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'possibly_sensitive': False,\n",
       " 'lang': 'qme',\n",
       " 'conversation_id': '1603565869673873408',\n",
       " 'referenced_tweets': [{'type': 'replied_to', 'id': '1603565869673873408'}],\n",
       " 'edit_history_tweet_ids': ['1608261522870136833'],\n",
       " 'reply_settings': 'everyone',\n",
       " 'created_at': '2022-12-29T00:40:01.000Z',\n",
       " 'public_metrics': {'retweet_count': 0,\n",
       "  'reply_count': 0,\n",
       "  'like_count': 0,\n",
       "  'quote_count': 0,\n",
       "  'impression_count': 3},\n",
       " 'entities': {'urls': [{'start': 10,\n",
       "    'end': 33,\n",
       "    'url': 'https://t.co/stR6weHTUO',\n",
       "    'expanded_url': 'https://rumble.com/v21y3qs-covid-19-vaccines-what-they-are-how-they-work-and-possible-causes-of-injuri.html',\n",
       "    'display_url': 'rumble.com/v21y3qs-covid-…',\n",
       "    'images': [{'url': 'https://pbs.twimg.com/news_img/1623388282397630467/SN1aV-kE?format=jpg&name=orig',\n",
       "      'width': 1280,\n",
       "      'height': 720},\n",
       "     {'url': 'https://pbs.twimg.com/news_img/1623388282397630467/SN1aV-kE?format=jpg&name=150x150',\n",
       "      'width': 150,\n",
       "      'height': 150}],\n",
       "    'status': 200,\n",
       "    'title': \"VSRF Reports: Highlights & exclusive interview footage with key doctors & speakers at Sen. Ron Johnson's Covid-19 Vaccine Roundtable\",\n",
       "    'description': \"Investigative journalist, Dan Cohen, interviewed key doctors and speakers for VSRF, for this 25 minute highlight reel which features both testimony and individual interviews from Senator Johnson's Rou\",\n",
       "    'unwound_url': 'https://rumble.com/v21y3qs-covid-19-vaccines-what-they-are-how-they-work-and-possible-causes-of-injuri.html'}],\n",
       "  'mentions': [{'start': 0,\n",
       "    'end': 9,\n",
       "    'username': 'aliceysu',\n",
       "    'id': '24709718'}]},\n",
       " 'in_reply_to_user_id': '24709718',\n",
       " 'text': '@aliceysu https://t.co/stR6weHTUO',\n",
       " 'edit_controls': {'edits_remaining': 5,\n",
       "  'is_edit_eligible': False,\n",
       "  'editable_until': '2022-12-29T01:10:01.000Z'},\n",
       " 'id': '1608261522870136833',\n",
       " 'author_id': '1519143967886954496'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c35a14cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6925\n",
      "10685\n"
     ]
    }
   ],
   "source": [
    "people = set()\n",
    "tweets = []\n",
    "for i in range(len(data)):\n",
    "    if (data[i]['conversation_id'] in user_ids):\n",
    "        people.add(data[i]['author_id'])\n",
    "        tweets.append(data[i]['author_id'])\n",
    "print(len(people))\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6864a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### INITIALISAING ACTIVITY TRACE DICTIONARY #########\n",
    "activity_traces = {}\n",
    "for element in user_ids:\n",
    "    activity_traces[element] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b30e6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 12, 29, 0, 40, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_format = \"%Y-%m-%dT%H:%M:%S\" \n",
    "datetime.strptime(data[0]['created_at'][:19], date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4efeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "######### ACCUMLATING TWEETS UNDER THEIR CONVERSATION ID #########\n",
    "count = 0\n",
    "for i in range(len(data)):\n",
    "    conversation_id = data[i]['conversation_id']\n",
    "    if conversation_id in user_ids:\n",
    "        if 'referenced_tweets' in data[i]:\n",
    "            if data[i]['referenced_tweets'][0]['type'] == 'replied_to':\n",
    "                if [data[i]['id'], datetime.strptime(data[i]['created_at'][:19], date_format), data[i]['author_id'], 1] in activity_traces[conversation_id]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    activity_traces[conversation_id].append([data[i]['id'], datetime.strptime(data[i]['created_at'][:19], date_format), data[i]['author_id'], 1])\n",
    "            elif data[i]['referenced_tweets'][0]['type'] == 'quoted':\n",
    "                if [data[i]['id'], datetime.strptime(data[i]['created_at'][:19], date_format), data[i]['author_id'], 2] in activity_traces[conversation_id]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    activity_traces[conversation_id].append([data[i]['id'], datetime.strptime(data[i]['created_at'][:19], date_format), data[i]['author_id'], 2])\n",
    "        else:\n",
    "            if [data[i]['id'], datetime.strptime(data[i]['created_at'][:19], date_format), data[i]['author_id'], 0] in activity_traces[conversation_id]:\n",
    "                count += 1\n",
    "            else:\n",
    "                activity_traces[conversation_id].append([data[i]['id'], datetime.strptime(data[i]['created_at'][:19], date_format), data[i]['author_id'], 0])\n",
    "            \n",
    "# 0 for original tweet, 1 for reply to another tweet, 2 for quoted tweet.\n",
    "# 54607 duplicated\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "009eccfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1608261522870136833',\n",
       "  datetime.datetime(2022, 12, 29, 0, 40, 1),\n",
       "  '1519143967886954496',\n",
       "  1],\n",
       " ['1603751795553361921',\n",
       "  datetime.datetime(2022, 12, 16, 13, 59, 58),\n",
       "  '2411054094',\n",
       "  1],\n",
       " ['1603566654969245696',\n",
       "  datetime.datetime(2022, 12, 16, 1, 44, 18),\n",
       "  '51081835',\n",
       "  1]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_traces['1603565869673873408']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c411e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### SORTING WITHIN AN ACTIVITY TRACE BASED ON TIME #########\n",
    "sorted_activity_trace_dirty = []\n",
    "for key in activity_traces:\n",
    "    sorted_activity_trace_dirty.append(sorted(activity_traces[key], key=lambda d: d[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa48a5b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######### REMOVING ACTIVITY TRACES THAT ARE OF LENGTH ONE OR LESS #########\n",
    "index = []\n",
    "sorted_activity_trace = []\n",
    "for i in range(len(sorted_activity_trace_dirty)):\n",
    "    if len(sorted_activity_trace_dirty[i]) > 1:\n",
    "        sorted_activity_trace.append(sorted_activity_trace_dirty[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f944d2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_activity_trace_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e94b48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### CREATING USER ID MAPPING AND THE REVERSE MAPPING TO OBTAIN INPUT MARKS VALUES AND DECODE THEM #########\n",
    "ents = []\n",
    "for actTrace in sorted_activity_trace:\n",
    "    for act in actTrace:\n",
    "        ents.append(act[2])\n",
    "\n",
    "idmap = {}\n",
    "other_way = {}\n",
    "for idx, ent in enumerate(set(ents)):\n",
    "    idmap[ent] = idx\n",
    "    other_way[idx] = ent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dadb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "numTraces = len(sorted_activity_trace)\n",
    "\n",
    "np.random.shuffle(sorted_activity_trace)\n",
    "\n",
    "valTraces = sorted_activity_trace[:int(0.1*numTraces)]\n",
    "testTraces = sorted_activity_trace[int(0.1*numTraces):int(0.2*numTraces)]\n",
    "trainTraces = sorted_activity_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92a6c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = {}\n",
    "cols = ['arrival_times', 'delta_times', 'marks', 'tweet_type']\n",
    "\n",
    "for col in cols:\n",
    "    split[col] = []\n",
    "\n",
    "tweetSeqMap = {}\n",
    "lengths = []\n",
    "\n",
    "for idx, activityTrace in enumerate(trainTraces): \n",
    "    #assert int(activityTrace[0][2]) == userId\n",
    "    \n",
    "    starts = [act[1] for act in activityTrace]\n",
    "    iStart = starts[0]\n",
    "    \n",
    "    normStarts = [ (act[1] - iStart).total_seconds()/(24*3600) for act in activityTrace]\n",
    "    assert normStarts[0] == 0\n",
    "    deltaTimes = [1.0] \n",
    "    for i in range(1,len(normStarts)):\n",
    "        deltaTimes.append(normStarts[i] - normStarts[i-1] )\n",
    "        \n",
    "    marks = []\n",
    "    for i in range(len(normStarts)):\n",
    "        marks.append(idmap[activityTrace[i][2]])\n",
    "    \n",
    "    tweet_type = []\n",
    "    for i in range(len(normStarts)):\n",
    "        tweet_type.append(activityTrace[i][3])\n",
    "        \n",
    "    tweetSeqMap[activityTrace[0][0]] = idx\n",
    "    lengths.append(len(normStarts))\n",
    "    split['arrival_times'].append(normStarts)\n",
    "    split['delta_times'].append(deltaTimes)\n",
    "    split['marks'].append(marks)\n",
    "    split['tweet_type'].append(tweet_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54b3d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "split1 = {}\n",
    "for col in cols:\n",
    "    split1[col] = []\n",
    "for idx, activityTrace in enumerate(testTraces): \n",
    "    #assert int(activityTrace[0][2]) == userId\n",
    "    \n",
    "    starts = [act[1] for act in activityTrace]\n",
    "    iStart = starts[0]\n",
    "    \n",
    "    normStarts = [ (act[1] - iStart).total_seconds()/(24*3600) for act in activityTrace]\n",
    "    assert normStarts[0] == 0\n",
    "    deltaTimes = [1.0] \n",
    "    for i in range(1,len(normStarts)):\n",
    "        deltaTimes.append( normStarts[i] - normStarts[i-1] )\n",
    "        \n",
    "    marks = []\n",
    "    for i in range(len(normStarts)):\n",
    "        marks.append(idmap[activityTrace[i][2]])\n",
    "        \n",
    "    tweet_type = []\n",
    "    for i in range(len(normStarts)):\n",
    "        tweet_type.append(activityTrace[i][3])\n",
    "        \n",
    "    tweetSeqMap[activityTrace[0][0]] = idx\n",
    "    \n",
    "    split1['arrival_times'].append(normStarts)\n",
    "    split1['delta_times'].append(deltaTimes)\n",
    "    split1['marks'].append(marks)\n",
    "    split1['tweet_type'].append(tweet_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "657feab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split2 = {}\n",
    "\n",
    "for col in cols:\n",
    "    split2[col] = []\n",
    "for idx, activityTrace in enumerate(valTraces): \n",
    "    #assert int(activityTrace[0][2]) == userId\n",
    "    \n",
    "    starts = [act[1] for act in activityTrace]\n",
    "    iStart = starts[0]\n",
    "    \n",
    "    normStarts = [ (act[1] - iStart).total_seconds()/(24*3600) for act in activityTrace]\n",
    "    assert normStarts[0] == 0\n",
    "    deltaTimes = [1.0] \n",
    "    for i in range(1,len(normStarts)):\n",
    "        deltaTimes.append( normStarts[i] - normStarts[i-1] )\n",
    "        \n",
    "    marks = []\n",
    "    for i in range(len(normStarts)):\n",
    "        marks.append(idmap[activityTrace[i][2]])\n",
    "        \n",
    "    tweet_type = []\n",
    "    for i in range(len(normStarts)):\n",
    "        tweet_type.append(activityTrace[i][3])\n",
    "    tweetSeqMap[activityTrace[0][0]] = idx\n",
    "\n",
    "    split2['arrival_times'].append(normStarts)\n",
    "    split2['delta_times'].append(deltaTimes)\n",
    "    split2['marks'].append(marks)\n",
    "    split2['tweet_type'].append(tweet_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7533e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_result = {}\n",
    "data_result[\"train\"] = split\n",
    "data_result[\"test\"] = split1\n",
    "data_result[\"dev\"] = split2\n",
    "data_result['dim_process'] = len(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4612f575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arrival_times': [[0.0, 2.527210648148148],\n",
       "  [0.0, 0.0014467592592592592],\n",
       "  [0.0, 0.03166666666666667, 30.01508101851852, 437.3130092592593],\n",
       "  [0.0, 0.14180555555555555],\n",
       "  [0.0,\n",
       "   0.006423611111111111,\n",
       "   0.01766203703703704,\n",
       "   0.02746527777777778,\n",
       "   0.07740740740740741,\n",
       "   0.08822916666666666,\n",
       "   0.19037037037037038,\n",
       "   0.4257523148148148,\n",
       "   0.5247685185185185,\n",
       "   0.5637152777777777,\n",
       "   0.66875,\n",
       "   1.6597337962962964,\n",
       "   1.6610763888888889,\n",
       "   1.7932291666666667],\n",
       "  [0.0,\n",
       "   0.09506944444444444,\n",
       "   0.3719675925925926,\n",
       "   0.421875,\n",
       "   0.8337037037037037,\n",
       "   1.1365277777777778,\n",
       "   3.1604282407407407,\n",
       "   3.6797569444444442,\n",
       "   6.233449074074074],\n",
       "  [0.0, 0.22703703703703704, 1.990324074074074, 2.297083333333333],\n",
       "  [0.0,\n",
       "   0.0030555555555555557,\n",
       "   0.3507986111111111,\n",
       "   0.6315046296296296,\n",
       "   2.8250462962962963],\n",
       "  [0.0,\n",
       "   0.0012731481481481483,\n",
       "   0.004340277777777778,\n",
       "   0.005613425925925926,\n",
       "   0.012326388888888888,\n",
       "   0.01238425925925926,\n",
       "   0.014293981481481482,\n",
       "   0.014421296296296297,\n",
       "   0.01670138888888889,\n",
       "   0.018703703703703705,\n",
       "   0.020277777777777777,\n",
       "   0.0259375,\n",
       "   0.026828703703703705,\n",
       "   0.027141203703703702,\n",
       "   0.03224537037037037,\n",
       "   0.03497685185185185,\n",
       "   0.04017361111111111,\n",
       "   0.048414351851851854,\n",
       "   0.36212962962962963,\n",
       "   0.3629398148148148,\n",
       "   0.6933333333333334],\n",
       "  [0.0, 1.2914467592592593],\n",
       "  [0.0,\n",
       "   0.0014583333333333334,\n",
       "   0.12026620370370371,\n",
       "   0.18835648148148149,\n",
       "   0.2044675925925926],\n",
       "  [0.0,\n",
       "   0.07358796296296297,\n",
       "   0.07893518518518519,\n",
       "   0.0999537037037037,\n",
       "   0.12357638888888889,\n",
       "   0.13644675925925925,\n",
       "   0.16424768518518518,\n",
       "   0.17622685185185186,\n",
       "   0.21410879629629628,\n",
       "   0.21920138888888888,\n",
       "   0.2576041666666667,\n",
       "   0.4295949074074074,\n",
       "   0.435,\n",
       "   0.44600694444444444,\n",
       "   0.4505439814814815,\n",
       "   0.48703703703703705,\n",
       "   0.8718402777777777,\n",
       "   2.0133564814814813,\n",
       "   2.880138888888889],\n",
       "  [0.0,\n",
       "   0.0028587962962962963,\n",
       "   0.01923611111111111,\n",
       "   0.026608796296296297,\n",
       "   0.027997685185185184,\n",
       "   0.031747685185185184,\n",
       "   0.03234953703703704,\n",
       "   0.03439814814814815,\n",
       "   0.03469907407407408,\n",
       "   0.04780092592592593,\n",
       "   0.06357638888888889,\n",
       "   0.06960648148148148,\n",
       "   0.07405092592592592,\n",
       "   0.07916666666666666,\n",
       "   0.10730324074074074,\n",
       "   0.1399537037037037,\n",
       "   0.15505787037037036,\n",
       "   0.21010416666666668,\n",
       "   0.24189814814814814,\n",
       "   0.3577314814814815,\n",
       "   0.360787037037037,\n",
       "   0.4763425925925926,\n",
       "   0.47944444444444445,\n",
       "   0.4831828703703704,\n",
       "   0.4905439814814815,\n",
       "   0.4905787037037037,\n",
       "   0.5326273148148148,\n",
       "   0.5332523148148148,\n",
       "   0.5344097222222223,\n",
       "   0.5358449074074074,\n",
       "   0.5387037037037037,\n",
       "   0.5392361111111111,\n",
       "   0.542025462962963,\n",
       "   0.5566782407407408,\n",
       "   0.5589236111111111,\n",
       "   0.5594791666666666,\n",
       "   0.5598958333333334,\n",
       "   0.5614467592592592,\n",
       "   0.5680439814814815,\n",
       "   0.5688194444444444,\n",
       "   0.5737962962962962,\n",
       "   0.6322569444444445,\n",
       "   0.6353356481481481,\n",
       "   0.6408101851851852,\n",
       "   0.6451157407407407,\n",
       "   0.6519560185185185,\n",
       "   0.6632291666666666,\n",
       "   0.7922222222222223,\n",
       "   0.905787037037037,\n",
       "   0.9570717592592592,\n",
       "   0.9598842592592592,\n",
       "   0.9820717592592593,\n",
       "   1.0516203703703704,\n",
       "   1.9940625,\n",
       "   2.1063310185185187],\n",
       "  [0.0, 0.09349537037037037, 1.5378472222222221],\n",
       "  [0.0,\n",
       "   0.008055555555555555,\n",
       "   0.011666666666666667,\n",
       "   0.018912037037037036,\n",
       "   0.05380787037037037,\n",
       "   0.06641203703703703,\n",
       "   0.06831018518518518,\n",
       "   0.07061342592592593,\n",
       "   0.07851851851851852,\n",
       "   0.10209490740740741,\n",
       "   0.11195601851851852,\n",
       "   0.1467013888888889,\n",
       "   0.15108796296296295,\n",
       "   0.17127314814814815,\n",
       "   0.17173611111111112,\n",
       "   0.17207175925925927,\n",
       "   0.23625,\n",
       "   0.25,\n",
       "   0.31506944444444446,\n",
       "   0.37298611111111113,\n",
       "   0.510775462962963,\n",
       "   0.5426504629629629,\n",
       "   0.5509722222222222,\n",
       "   0.7496527777777777,\n",
       "   0.7653935185185186,\n",
       "   3.5878587962962962,\n",
       "   4.8875,\n",
       "   15.03761574074074,\n",
       "   61.99692129629629],\n",
       "  [0.0, 0.12893518518518518, 0.23189814814814816, 0.24606481481481482],\n",
       "  [0.0,\n",
       "   0.008368055555555556,\n",
       "   0.01193287037037037,\n",
       "   0.07388888888888889,\n",
       "   0.26109953703703703,\n",
       "   0.587037037037037],\n",
       "  [0.0,\n",
       "   0.0,\n",
       "   1.1574074074074073e-05,\n",
       "   1.1574074074074073e-05,\n",
       "   2.3148148148148147e-05,\n",
       "   0.002395833333333333,\n",
       "   0.05412037037037037,\n",
       "   0.057708333333333334,\n",
       "   0.08206018518518518,\n",
       "   0.08546296296296296,\n",
       "   0.0858912037037037,\n",
       "   0.35359953703703706,\n",
       "   0.48527777777777775,\n",
       "   0.5038541666666667,\n",
       "   0.6445601851851852,\n",
       "   0.6459722222222222,\n",
       "   2.2883449074074074],\n",
       "  [0.0,\n",
       "   0.01099537037037037,\n",
       "   0.22769675925925925,\n",
       "   0.2528125,\n",
       "   0.3630787037037037,\n",
       "   0.36420138888888887,\n",
       "   0.5364467592592592,\n",
       "   0.591099537037037,\n",
       "   0.5925462962962963,\n",
       "   0.8244328703703704,\n",
       "   0.9525694444444445,\n",
       "   2.431064814814815,\n",
       "   2.444328703703704,\n",
       "   3.246226851851852,\n",
       "   3.2472222222222222],\n",
       "  [0.0,\n",
       "   0.000925925925925926,\n",
       "   0.004097222222222223,\n",
       "   0.004097222222222223,\n",
       "   0.009699074074074074,\n",
       "   0.009710648148148149],\n",
       "  [0.0,\n",
       "   0.05178240740740741,\n",
       "   0.06288194444444445,\n",
       "   47.168645833333336,\n",
       "   47.435763888888886,\n",
       "   49.02096064814815,\n",
       "   52.60402777777778],\n",
       "  [0.0, 0.7056944444444444],\n",
       "  [0.0,\n",
       "   0.02863425925925926,\n",
       "   0.042199074074074076,\n",
       "   0.1655787037037037,\n",
       "   1.2891319444444445,\n",
       "   1.3773148148148149],\n",
       "  [0.0, 0.3735648148148148],\n",
       "  [0.0, 30.755034722222224],\n",
       "  [0.0, 0.24688657407407408],\n",
       "  [0.0,\n",
       "   0.0002199074074074074,\n",
       "   0.0052662037037037035,\n",
       "   0.1437037037037037,\n",
       "   0.16010416666666666,\n",
       "   0.20019675925925925,\n",
       "   0.21362268518518518,\n",
       "   0.2601967592592593,\n",
       "   0.2896412037037037,\n",
       "   0.33144675925925926,\n",
       "   0.3930439814814815,\n",
       "   0.4248958333333333,\n",
       "   0.4292592592592593,\n",
       "   0.4529166666666667,\n",
       "   0.4541203703703704,\n",
       "   0.45875,\n",
       "   0.4619097222222222,\n",
       "   0.46921296296296294,\n",
       "   0.6817361111111111,\n",
       "   0.6974074074074074,\n",
       "   0.6982523148148149,\n",
       "   0.9073726851851852,\n",
       "   0.9109027777777777,\n",
       "   0.9113310185185185,\n",
       "   0.9194675925925926,\n",
       "   0.9568287037037037,\n",
       "   0.9575115740740741,\n",
       "   0.9585185185185185,\n",
       "   1.0096643518518518,\n",
       "   1.401099537037037,\n",
       "   3.7693287037037035,\n",
       "   4.095289351851852,\n",
       "   5.510636574074074,\n",
       "   5.514421296296296,\n",
       "   5.528414351851852,\n",
       "   5.556875,\n",
       "   5.584722222222222,\n",
       "   5.615277777777778,\n",
       "   5.627476851851852,\n",
       "   5.642002314814815,\n",
       "   5.643368055555555,\n",
       "   5.646678240740741,\n",
       "   5.650925925925926,\n",
       "   5.653101851851852,\n",
       "   5.654525462962963,\n",
       "   5.659918981481481,\n",
       "   5.676099537037037,\n",
       "   5.847037037037037,\n",
       "   5.8696064814814815,\n",
       "   5.876400462962963,\n",
       "   5.8794560185185185,\n",
       "   5.968530092592593,\n",
       "   5.972986111111111,\n",
       "   5.983402777777778,\n",
       "   6.248252314814815,\n",
       "   6.250243055555556,\n",
       "   6.252962962962963,\n",
       "   6.368969907407408,\n",
       "   6.373055555555555,\n",
       "   6.376956018518518,\n",
       "   6.5511689814814815,\n",
       "   6.553819444444445,\n",
       "   6.555706018518518,\n",
       "   6.559097222222222,\n",
       "   6.560069444444444,\n",
       "   6.572418981481482,\n",
       "   6.574884259259259,\n",
       "   6.576006944444444,\n",
       "   6.585601851851852,\n",
       "   6.851354166666667,\n",
       "   6.861354166666667,\n",
       "   6.863958333333334,\n",
       "   6.866099537037037,\n",
       "   6.867835648148148,\n",
       "   6.868576388888889,\n",
       "   6.974826388888889,\n",
       "   7.081851851851852,\n",
       "   7.413356481481482,\n",
       "   16.522222222222222,\n",
       "   17.84835648148148,\n",
       "   34.73021990740741,\n",
       "   38.60697916666667,\n",
       "   42.62244212962963,\n",
       "   45.019050925925924,\n",
       "   47.47,\n",
       "   49.373206018518516,\n",
       "   49.37469907407407,\n",
       "   50.51050925925926,\n",
       "   51.279212962962966,\n",
       "   52.34934027777778,\n",
       "   52.35295138888889,\n",
       "   53.308761574074076],\n",
       "  [0.0,\n",
       "   0.00045138888888888887,\n",
       "   0.007858796296296296,\n",
       "   0.04400462962962963,\n",
       "   0.04407407407407407,\n",
       "   0.07799768518518518],\n",
       "  [0.0, 0.011006944444444444, 0.011585648148148149, 0.012314814814814815]],\n",
       " 'delta_times': [[1.0, 2.527210648148148],\n",
       "  [1.0, 0.0014467592592592592],\n",
       "  [1.0, 0.03166666666666667, 29.983414351851852, 407.29792824074076],\n",
       "  [1.0, 0.14180555555555555],\n",
       "  [1.0,\n",
       "   0.006423611111111111,\n",
       "   0.011238425925925928,\n",
       "   0.00980324074074074,\n",
       "   0.04994212962962963,\n",
       "   0.010821759259259253,\n",
       "   0.10214120370370372,\n",
       "   0.2353819444444444,\n",
       "   0.09901620370370368,\n",
       "   0.03894675925925928,\n",
       "   0.10503472222222221,\n",
       "   0.9909837962962964,\n",
       "   0.0013425925925925064,\n",
       "   0.13215277777777779],\n",
       "  [1.0,\n",
       "   0.09506944444444444,\n",
       "   0.27689814814814817,\n",
       "   0.0499074074074074,\n",
       "   0.4118287037037037,\n",
       "   0.3028240740740741,\n",
       "   2.023900462962963,\n",
       "   0.5193287037037035,\n",
       "   2.55369212962963],\n",
       "  [1.0, 0.22703703703703704, 1.763287037037037, 0.3067592592592592],\n",
       "  [1.0,\n",
       "   0.0030555555555555557,\n",
       "   0.34774305555555557,\n",
       "   0.2807060185185185,\n",
       "   2.1935416666666665],\n",
       "  [1.0,\n",
       "   0.0012731481481481483,\n",
       "   0.0030671296296296297,\n",
       "   0.0012731481481481483,\n",
       "   0.006712962962962962,\n",
       "   5.787037037037132e-05,\n",
       "   0.0019097222222222224,\n",
       "   0.00012731481481481448,\n",
       "   0.002280092592592594,\n",
       "   0.0020023148148148144,\n",
       "   0.0015740740740740715,\n",
       "   0.005659722222222222,\n",
       "   0.0008912037037037066,\n",
       "   0.0003124999999999968,\n",
       "   0.005104166666666667,\n",
       "   0.0027314814814814806,\n",
       "   0.005196759259259262,\n",
       "   0.008240740740740743,\n",
       "   0.3137152777777778,\n",
       "   0.0008101851851851638,\n",
       "   0.33039351851851856],\n",
       "  [1.0, 1.2914467592592593],\n",
       "  [1.0,\n",
       "   0.0014583333333333334,\n",
       "   0.11880787037037037,\n",
       "   0.06809027777777778,\n",
       "   0.016111111111111104],\n",
       "  [1.0,\n",
       "   0.07358796296296297,\n",
       "   0.005347222222222225,\n",
       "   0.021018518518518506,\n",
       "   0.02362268518518519,\n",
       "   0.012870370370370365,\n",
       "   0.027800925925925923,\n",
       "   0.01197916666666668,\n",
       "   0.037881944444444426,\n",
       "   0.005092592592592593,\n",
       "   0.03840277777777781,\n",
       "   0.17199074074074072,\n",
       "   0.005405092592592586,\n",
       "   0.011006944444444444,\n",
       "   0.004537037037037062,\n",
       "   0.03649305555555554,\n",
       "   0.3848032407407407,\n",
       "   1.1415162037037034,\n",
       "   0.8667824074074075],\n",
       "  [1.0,\n",
       "   0.0028587962962962963,\n",
       "   0.016377314814814813,\n",
       "   0.007372685185185187,\n",
       "   0.0013888888888888874,\n",
       "   0.00375,\n",
       "   0.0006018518518518534,\n",
       "   0.002048611111111112,\n",
       "   0.0003009259259259267,\n",
       "   0.01310185185185185,\n",
       "   0.015775462962962963,\n",
       "   0.006030092592592587,\n",
       "   0.004444444444444445,\n",
       "   0.00511574074074074,\n",
       "   0.028136574074074078,\n",
       "   0.03265046296296295,\n",
       "   0.015104166666666669,\n",
       "   0.055046296296296315,\n",
       "   0.031793981481481465,\n",
       "   0.11583333333333334,\n",
       "   0.0030555555555555336,\n",
       "   0.11555555555555558,\n",
       "   0.0031018518518518556,\n",
       "   0.0037384259259259367,\n",
       "   0.007361111111111096,\n",
       "   3.472222222222765e-05,\n",
       "   0.042048611111111134,\n",
       "   0.0006249999999999867,\n",
       "   0.0011574074074074403,\n",
       "   0.0014351851851851505,\n",
       "   0.002858796296296262,\n",
       "   0.0005324074074074536,\n",
       "   0.002789351851851807,\n",
       "   0.014652777777777848,\n",
       "   0.0022453703703703143,\n",
       "   0.0005555555555555314,\n",
       "   0.0004166666666667318,\n",
       "   0.0015509259259258723,\n",
       "   0.006597222222222254,\n",
       "   0.0007754629629629362,\n",
       "   0.004976851851851816,\n",
       "   0.05846064814814822,\n",
       "   0.003078703703703667,\n",
       "   0.005474537037037042,\n",
       "   0.0043055555555555625,\n",
       "   0.006840277777777737,\n",
       "   0.011273148148148171,\n",
       "   0.12899305555555562,\n",
       "   0.11356481481481473,\n",
       "   0.051284722222222245,\n",
       "   0.0028124999999999956,\n",
       "   0.022187500000000027,\n",
       "   0.0695486111111111,\n",
       "   0.9424421296296297,\n",
       "   0.1122685185185186],\n",
       "  [1.0, 0.09349537037037037, 1.4443518518518519],\n",
       "  [1.0,\n",
       "   0.008055555555555555,\n",
       "   0.003611111111111112,\n",
       "   0.007245370370370369,\n",
       "   0.034895833333333334,\n",
       "   0.012604166666666666,\n",
       "   0.0018981481481481488,\n",
       "   0.0023032407407407446,\n",
       "   0.007905092592592589,\n",
       "   0.023576388888888897,\n",
       "   0.009861111111111112,\n",
       "   0.03474537037037037,\n",
       "   0.004386574074074057,\n",
       "   0.020185185185185195,\n",
       "   0.00046296296296297057,\n",
       "   0.00033564814814815436,\n",
       "   0.06417824074074072,\n",
       "   0.013750000000000012,\n",
       "   0.06506944444444446,\n",
       "   0.05791666666666667,\n",
       "   0.13778935185185182,\n",
       "   0.03187499999999999,\n",
       "   0.008321759259259265,\n",
       "   0.19868055555555553,\n",
       "   0.015740740740740833,\n",
       "   2.822465277777778,\n",
       "   1.299641203703704,\n",
       "   10.150115740740741,\n",
       "   46.95930555555555],\n",
       "  [1.0, 0.12893518518518518, 0.10296296296296298, 0.01416666666666666],\n",
       "  [1.0,\n",
       "   0.008368055555555556,\n",
       "   0.003564814814814814,\n",
       "   0.06195601851851852,\n",
       "   0.18721064814814814,\n",
       "   0.3259375],\n",
       "  [1.0,\n",
       "   0.0,\n",
       "   1.1574074074074073e-05,\n",
       "   0.0,\n",
       "   1.1574074074074073e-05,\n",
       "   0.002372685185185185,\n",
       "   0.051724537037037034,\n",
       "   0.0035879629629629664,\n",
       "   0.024351851851851847,\n",
       "   0.0034027777777777823,\n",
       "   0.0004282407407407429,\n",
       "   0.2677083333333333,\n",
       "   0.1316782407407407,\n",
       "   0.01857638888888896,\n",
       "   0.1407060185185185,\n",
       "   0.0014120370370369617,\n",
       "   1.6423726851851852],\n",
       "  [1.0,\n",
       "   0.01099537037037037,\n",
       "   0.21670138888888887,\n",
       "   0.025115740740740744,\n",
       "   0.11026620370370371,\n",
       "   0.0011226851851851571,\n",
       "   0.17224537037037035,\n",
       "   0.05465277777777777,\n",
       "   0.0014467592592593004,\n",
       "   0.2318865740740741,\n",
       "   0.12813657407407408,\n",
       "   1.4784953703703705,\n",
       "   0.013263888888888964,\n",
       "   0.801898148148148,\n",
       "   0.000995370370370452],\n",
       "  [1.0,\n",
       "   0.000925925925925926,\n",
       "   0.0031712962962962966,\n",
       "   0.0,\n",
       "   0.005601851851851851,\n",
       "   1.1574074074075305e-05],\n",
       "  [1.0,\n",
       "   0.05178240740740741,\n",
       "   0.01109953703703704,\n",
       "   47.105763888888895,\n",
       "   0.26711805555554946,\n",
       "   1.5851967592592615,\n",
       "   3.583067129629633],\n",
       "  [1.0, 0.7056944444444444],\n",
       "  [1.0,\n",
       "   0.02863425925925926,\n",
       "   0.013564814814814818,\n",
       "   0.12337962962962962,\n",
       "   1.1235532407407407,\n",
       "   0.08818287037037043],\n",
       "  [1.0, 0.3735648148148148],\n",
       "  [1.0, 30.755034722222224],\n",
       "  [1.0, 0.24688657407407408],\n",
       "  [1.0,\n",
       "   0.0002199074074074074,\n",
       "   0.005046296296296296,\n",
       "   0.1384375,\n",
       "   0.016400462962962964,\n",
       "   0.040092592592592596,\n",
       "   0.013425925925925924,\n",
       "   0.0465740740740741,\n",
       "   0.02944444444444444,\n",
       "   0.04180555555555554,\n",
       "   0.06159722222222225,\n",
       "   0.0318518518518518,\n",
       "   0.004363425925925979,\n",
       "   0.023657407407407405,\n",
       "   0.0012037037037037068,\n",
       "   0.004629629629629595,\n",
       "   0.0031597222222222165,\n",
       "   0.007303240740740735,\n",
       "   0.21252314814814816,\n",
       "   0.015671296296296267,\n",
       "   0.0008449074074075025,\n",
       "   0.20912037037037035,\n",
       "   0.0035300925925925153,\n",
       "   0.0004282407407407707,\n",
       "   0.008136574074074088,\n",
       "   0.03736111111111107,\n",
       "   0.0006828703703704031,\n",
       "   0.0010069444444444908,\n",
       "   0.051145833333333224,\n",
       "   0.39143518518518516,\n",
       "   2.3682291666666666,\n",
       "   0.3259606481481483,\n",
       "   1.4153472222222225,\n",
       "   0.0037847222222220367,\n",
       "   0.013993055555555856,\n",
       "   0.028460648148147527,\n",
       "   0.027847222222222356,\n",
       "   0.03055555555555589,\n",
       "   0.012199074074073835,\n",
       "   0.014525462962962976,\n",
       "   0.0013657407407405842,\n",
       "   0.0033101851851853326,\n",
       "   0.004247685185185368,\n",
       "   0.002175925925925526,\n",
       "   0.0014236111111110006,\n",
       "   0.0053935185185185475,\n",
       "   0.016180555555555642,\n",
       "   0.17093749999999996,\n",
       "   0.022569444444444642,\n",
       "   0.006793981481481914,\n",
       "   0.003055555555555145,\n",
       "   0.08907407407407408,\n",
       "   0.004456018518518512,\n",
       "   0.010416666666666963,\n",
       "   0.26484953703703695,\n",
       "   0.001990740740740904,\n",
       "   0.0027199074074069074,\n",
       "   0.11600694444444493,\n",
       "   0.0040856481481474916,\n",
       "   0.0039004629629628695,\n",
       "   0.1742129629629634,\n",
       "   0.0026504629629631182,\n",
       "   0.0018865740740734438,\n",
       "   0.003391203703704271,\n",
       "   0.0009722222222219301,\n",
       "   0.01234953703703745,\n",
       "   0.002465277777777608,\n",
       "   0.0011226851851846575,\n",
       "   0.00959490740740776,\n",
       "   0.2657523148148151,\n",
       "   0.009999999999999787,\n",
       "   0.0026041666666669627,\n",
       "   0.0021412037037036313,\n",
       "   0.0017361111111107164,\n",
       "   0.0007407407407411526,\n",
       "   0.10625000000000018,\n",
       "   0.10702546296296234,\n",
       "   0.33150462962963,\n",
       "   9.10886574074074,\n",
       "   1.326134259259259,\n",
       "   16.88186342592593,\n",
       "   3.8767592592592592,\n",
       "   4.015462962962964,\n",
       "   2.3966087962962916,\n",
       "   2.4509490740740745,\n",
       "   1.9032060185185173,\n",
       "   0.001493055555556566,\n",
       "   1.1358101851851856,\n",
       "   0.7687037037037072,\n",
       "   1.0701273148148118,\n",
       "   0.0036111111111125638,\n",
       "   0.9558101851851859],\n",
       "  [1.0,\n",
       "   0.00045138888888888887,\n",
       "   0.007407407407407407,\n",
       "   0.036145833333333335,\n",
       "   6.944444444444142e-05,\n",
       "   0.03392361111111111],\n",
       "  [1.0, 0.011006944444444444, 0.0005787037037037045, 0.0007291666666666662]],\n",
       " 'marks': [[3060, 5080],\n",
       "  [4288, 3044],\n",
       "  [2541, 2541, 468, 4397],\n",
       "  [2541, 1985],\n",
       "  [2444,\n",
       "   4252,\n",
       "   1971,\n",
       "   5645,\n",
       "   4415,\n",
       "   6212,\n",
       "   387,\n",
       "   5742,\n",
       "   282,\n",
       "   457,\n",
       "   5645,\n",
       "   619,\n",
       "   619,\n",
       "   913],\n",
       "  [2541, 2541, 48, 4673, 1665, 4404, 993, 993, 4365],\n",
       "  [1255, 6420, 2290, 2901],\n",
       "  [2231, 6496, 4117, 1556, 4987],\n",
       "  [2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   1292,\n",
       "   2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   3074,\n",
       "   2541,\n",
       "   3074,\n",
       "   3074,\n",
       "   3074,\n",
       "   3074,\n",
       "   6696,\n",
       "   6696,\n",
       "   5785],\n",
       "  [1043, 5413],\n",
       "  [2541, 5617, 3646, 24, 5203],\n",
       "  [4862,\n",
       "   3951,\n",
       "   3577,\n",
       "   1607,\n",
       "   2118,\n",
       "   6233,\n",
       "   5445,\n",
       "   1144,\n",
       "   6448,\n",
       "   203,\n",
       "   6732,\n",
       "   1480,\n",
       "   5083,\n",
       "   1479,\n",
       "   1480,\n",
       "   5083,\n",
       "   6690,\n",
       "   2469,\n",
       "   4907],\n",
       "  [2541,\n",
       "   2541,\n",
       "   1086,\n",
       "   5206,\n",
       "   5206,\n",
       "   5206,\n",
       "   5206,\n",
       "   2143,\n",
       "   2143,\n",
       "   5310,\n",
       "   2767,\n",
       "   6401,\n",
       "   5781,\n",
       "   5781,\n",
       "   1165,\n",
       "   5176,\n",
       "   5206,\n",
       "   855,\n",
       "   1265,\n",
       "   5920,\n",
       "   5920,\n",
       "   5854,\n",
       "   1944,\n",
       "   111,\n",
       "   4361,\n",
       "   6472,\n",
       "   771,\n",
       "   771,\n",
       "   2204,\n",
       "   771,\n",
       "   771,\n",
       "   2204,\n",
       "   771,\n",
       "   5720,\n",
       "   5854,\n",
       "   771,\n",
       "   771,\n",
       "   771,\n",
       "   5854,\n",
       "   771,\n",
       "   771,\n",
       "   4378,\n",
       "   4378,\n",
       "   4378,\n",
       "   4378,\n",
       "   4378,\n",
       "   6671,\n",
       "   3396,\n",
       "   3683,\n",
       "   1404,\n",
       "   392,\n",
       "   2165,\n",
       "   392,\n",
       "   4560,\n",
       "   4003],\n",
       "  [2541, 2420, 4939],\n",
       "  [3748,\n",
       "   5702,\n",
       "   5752,\n",
       "   3441,\n",
       "   4531,\n",
       "   3441,\n",
       "   5752,\n",
       "   5752,\n",
       "   6697,\n",
       "   6777,\n",
       "   3441,\n",
       "   5752,\n",
       "   3441,\n",
       "   3809,\n",
       "   3809,\n",
       "   3809,\n",
       "   2451,\n",
       "   6151,\n",
       "   6626,\n",
       "   220,\n",
       "   5752,\n",
       "   5752,\n",
       "   6268,\n",
       "   5270,\n",
       "   5983,\n",
       "   1140,\n",
       "   3312,\n",
       "   6697,\n",
       "   2284],\n",
       "  [2437, 3224, 6188, 3134],\n",
       "  [3947, 3200, 6271, 2311, 3277, 588],\n",
       "  [2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   2541,\n",
       "   4670,\n",
       "   6779,\n",
       "   2541,\n",
       "   3581,\n",
       "   3581,\n",
       "   2659,\n",
       "   2541,\n",
       "   2684,\n",
       "   2541,\n",
       "   2541,\n",
       "   4604],\n",
       "  [3528,\n",
       "   1283,\n",
       "   6331,\n",
       "   639,\n",
       "   84,\n",
       "   84,\n",
       "   4330,\n",
       "   6028,\n",
       "   6028,\n",
       "   2756,\n",
       "   6268,\n",
       "   946,\n",
       "   946,\n",
       "   593,\n",
       "   593],\n",
       "  [2541, 4053, 2541, 2541, 2541, 2541],\n",
       "  [6426, 5860, 2400, 3294, 1031, 3343, 5066],\n",
       "  [2030, 6796],\n",
       "  [2541, 2219, 1273, 4969, 1475, 3878],\n",
       "  [6065, 3701],\n",
       "  [5691, 468],\n",
       "  [2541, 5082],\n",
       "  [3024,\n",
       "   3024,\n",
       "   3451,\n",
       "   3669,\n",
       "   5754,\n",
       "   1424,\n",
       "   5754,\n",
       "   1108,\n",
       "   6210,\n",
       "   465,\n",
       "   663,\n",
       "   758,\n",
       "   4038,\n",
       "   2801,\n",
       "   5458,\n",
       "   1905,\n",
       "   5684,\n",
       "   6813,\n",
       "   3669,\n",
       "   4723,\n",
       "   4723,\n",
       "   3244,\n",
       "   1089,\n",
       "   1089,\n",
       "   1089,\n",
       "   3669,\n",
       "   3669,\n",
       "   3669,\n",
       "   3669,\n",
       "   1089,\n",
       "   753,\n",
       "   5212,\n",
       "   1177,\n",
       "   4699,\n",
       "   5458,\n",
       "   1177,\n",
       "   5458,\n",
       "   1177,\n",
       "   5458,\n",
       "   1177,\n",
       "   5458,\n",
       "   5458,\n",
       "   1177,\n",
       "   1177,\n",
       "   1177,\n",
       "   5458,\n",
       "   5458,\n",
       "   5684,\n",
       "   1177,\n",
       "   1177,\n",
       "   1177,\n",
       "   5458,\n",
       "   5458,\n",
       "   5458,\n",
       "   1177,\n",
       "   1177,\n",
       "   1177,\n",
       "   5458,\n",
       "   5458,\n",
       "   5458,\n",
       "   1177,\n",
       "   1177,\n",
       "   1177,\n",
       "   1177,\n",
       "   1177,\n",
       "   5458,\n",
       "   5458,\n",
       "   5458,\n",
       "   5458,\n",
       "   1177,\n",
       "   1177,\n",
       "   1177,\n",
       "   1177,\n",
       "   1177,\n",
       "   1177,\n",
       "   5458,\n",
       "   1177,\n",
       "   4699,\n",
       "   316,\n",
       "   2313,\n",
       "   3602,\n",
       "   4164,\n",
       "   2500,\n",
       "   6207,\n",
       "   5215,\n",
       "   615,\n",
       "   615,\n",
       "   5261,\n",
       "   101,\n",
       "   4175,\n",
       "   4175,\n",
       "   1182],\n",
       "  [4059, 5664, 5424, 4576, 4576, 4490],\n",
       "  [6402, 58, 1634, 2541]],\n",
       " 'tweet_type': [[1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],\n",
       "  [1, 1],\n",
       "  [1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [2, 1, 1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1, 1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_result['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3389cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('kdd_data/attempt_dev.pkl', 'rb') as f:\n",
    "    att = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17172b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['arrival_times', 'delta_times', 'marks', 'tweet_type'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att['dev'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7004124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in split['arrival_times']:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "80123d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3252"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6cbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
