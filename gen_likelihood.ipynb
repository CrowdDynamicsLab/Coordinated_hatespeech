{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d4ca75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from scipy.stats import beta\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import os\n",
    "import ast\n",
    "#from Content import *\n",
    "#from Venue import *\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dad03b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## People\n",
    "## Time\n",
    "## Topic\n",
    "## Media Contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14902ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './data'\n",
    "journalist = 'aliceysu'\n",
    "with open(os.path.join(out_dir, f'{journalist}_dict.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(out_dir, f'{journalist}_ids.pkl'), 'rb') as f:\n",
    "    map_id = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(out_dir, f'{journalist}_lan.pkl'), 'rb') as f:\n",
    "    map_lan = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(out_dir, f'{journalist}_type.pkl'), 'rb') as f:\n",
    "    map_type = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(out_dir, f'{journalist}_reply.pkl'), 'rb') as f:\n",
    "    map_reply = pickle.load(f)\n",
    "\n",
    "alice = pd.DataFrame.from_dict(data)\n",
    "alice_sort = alice.sort_values(by=['created_at'])\n",
    "conv = pd.read_csv(os.path.join(out_dir, f'{journalist}_conv_labels.csv'))\n",
    "\n",
    "data = pkl.load(open(os.path.join(out_dir, f'{journalist}_dict.pkl'), 'rb'))\n",
    "journal = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "43fff673",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "for item in conv['context']:\n",
    "    temp_s = ''\n",
    "    if item == '0':\n",
    "        contexts.append('Unified Twitter Taxonomy')\n",
    "\n",
    "    else:\n",
    "        for i in ast.literal_eval(item):\n",
    "            temp_s  += (i['domain']['name'] + ' ' + i['entity']['name'])\n",
    "        contexts.append(temp_s)\n",
    "    \n",
    "annotations = []\n",
    "for item in conv['annotations']:\n",
    "    tem_s = ''\n",
    "    if item == '0':\n",
    "        annotations.append('Others')\n",
    "\n",
    "    else:\n",
    "        for i in ast.literal_eval(item):\n",
    "            temp_s += i['type'] + ' ' + i['normalized_text']\n",
    "\n",
    "        annotations.append(temp_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "61512e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal['context'] = contexts\n",
    "journal['annotations'] = annotations\n",
    "journal_sort = journal.sort_values(by=['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "4780a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topics\n",
    "## people\n",
    "ref_ids = []\n",
    "\n",
    "journal_sort['topics'] = None\n",
    "journal_sort['ppls'] = None\n",
    "conv_ids = set(journal_sort['conversation_id'])\n",
    "for conv in list(conv_ids):\n",
    "    test_d = journal_sort[journal_sort['conversation_id'] == conv]\n",
    "    for index, item in test_d.iterrows():\n",
    "        i = 0\n",
    "        tweet_id = item['tweet_id']\n",
    "        ref_id = item['reference_id']\n",
    "        conv_id = item['conversation_id']\n",
    "        topic1 = item['context']\n",
    "        anno1 = item['annotations']\n",
    "        if len(test_d) == 1:\n",
    "            topic2 = 'Unified Twitter Taxonomy'\n",
    "            anno2 = 'Others'\n",
    "            journal_sort.at[index, 'topics']=[text1, text2]\n",
    "            journal_sort.at[index, 'ppls']=[anno1, anno2]\n",
    "            continue\n",
    "        if ref_id not in ref_ids:\n",
    "            if tweet_id == test_d.iloc[0]['tweet_id']:\n",
    "                text2 = test_d[test_d['tweet_id']==test_d.iloc[1]['tweet_id']]['context'].item()\n",
    "                anno2 = test_d[test_d['tweet_id']==test_d.iloc[1]['tweet_id']]['annotations'].item()\n",
    "            else:\n",
    "                text2 = test_d.iloc[i-1]['context']\n",
    "                anno2 = test_d.iloc[i-1]['annotations']\n",
    "        else:\n",
    "            #print(tweet_id,ref_id)\n",
    "            text2 = test_d[test_d['tweet_id']==ref_id]['context'].item()\n",
    "            anno2 = test_d[test_d['tweet_id']==ref_id]['annotations'].item()\n",
    "\n",
    "        ref_ids.append(tweet_id)\n",
    "        \n",
    "        journal_sort.at[index, 'topics']=[text1, text2]\n",
    "        journal_sort.at[index, 'ppls']=[anno1, anno2]\n",
    "        #print(text2, journal_sort.at[index, 'topics'])\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "8edc775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_sort.to_csv(os.path.join(out_dir, f'{journalist}_context.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "05fff4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b8e3b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "10632it [17:00, 10.41it/s]\n"
     ]
    }
   ],
   "source": [
    "context_sims = []\n",
    "anno_sims = []\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "for index, item in tqdm(journal_sort.iterrows()):\n",
    "    # Tokenize input text and convert to tensor\n",
    "    c1 = item['topics'][0]\n",
    "    c2 = item['topics'][1]\n",
    "    \n",
    "    a1 = item['ppls'][0]\n",
    "    a2 = item['ppls'][1]\n",
    "    inputs1 = tokenizer(c1, return_tensors=\"pt\")\n",
    "    inputs2 = tokenizer(c2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)\n",
    "        outputs2 = model(**inputs2)\n",
    "\n",
    "    # The last hidden state is the sequence of hidden states of the last layer of the model\n",
    "    last_hidden_states1 = outputs1.last_hidden_state\n",
    "    last_hidden_states2 = outputs2.last_hidden_state\n",
    "\n",
    "    # Optionally, use the [CLS] token's embedding as the representation for the entire sentence\n",
    "    sentence_embedding1 = last_hidden_states1[:, 0, :]\n",
    "    sentence_embedding2 = last_hidden_states2[:, 0, :]\n",
    "\n",
    "    cosine = np.dot(sentence_embedding1,sentence_embedding2.T)/(norm(sentence_embedding1)*norm(sentence_embedding2))\n",
    "    context_sims.append(cosine)\n",
    "    #print(sentence_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "b3e2824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_sort['topics_sim'] = context_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "39ec7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_sort.to_csv(os.path.join(out_dir, f'{journalist}_context.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa615078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: [[0.9301709]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    " \n",
    "# define two lists or array\n",
    "A = other_vec\n",
    "B = sentence_embedding\n",
    " \n",
    "# compute cosine similarity\n",
    "cosine = np.dot(A,B.T)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40f21c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[182.91821]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A,B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4efc5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Organization_vec = sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84e1dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_vec = sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88dc4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_place_vec = sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867368df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PA * Latest * Field * Self = 2*2*2*2 = 16\n",
    "## PA: Normal PA (p=0.1, p/#nodes + (1-p)indegree/sum_of_indegree), Uniform (1/#nodes)\n",
    "## Latest: Normal Latest (beta(10,1), x=1-(outyear-inyear)/(outyear-oldest_year)), Uniform (1/(outyear-oldest_year))\n",
    "## Field: Similar (1-(1-e^(-||x-y||_2))/(1-e^(-2)), x&y L2-normalized), Different ((1-e^(-||x-y||_2))/(1-e^(-2)), x&y L2-normalized)\n",
    "## Self: Prefer (coauthors: 0.9/#coauthors, non-coauthors: 0.1/#non-coauthors), Not Prefer (coauthors: 0.1/#coauthors, non-coauthors: 0.9/#non-coauthors)\n",
    "\n",
    "## 1st: Normal_PA * Normal_Latest * Similar_Field * Prefer_Self\n",
    "## 2nd: Normal_PA * Normal_Latest * Similar_Field * NotPrefer_Self\n",
    "## 3rd: Normal_PA * Normal_Latest * Different_Field * Prefer_Self\n",
    "## 4th: Normal_PA * Normal_Latest * Different_Field * NotPrefer_Self\n",
    "## 5th: Normal_PA * Uniform_Latest * Similar_Field * Prefer_Self\n",
    "## 6th: Normal_PA * Uniform_Latest * Similar_Field * NotPrefer_Self\n",
    "## 7th: Normal_PA * Uniform_Latest * Different_Field * Prefer_Self\n",
    "## 8th: Normal_PA * Uniform_Latest * Different_Field * NotPrefer_Self\n",
    "## 9th: Uniform_PA * Normal_Latest * Similar_Field * Prefer_Self\n",
    "## 10th: Uniform_PA * Normal_Latest * Similar_Field * NotPrefer_Self\n",
    "## 11th: Uniform_PA * Normal_Latest * Different_Field * Prefer_Self\n",
    "## 12th: Uniform_PA * Normal_Latest * Different_Field * NotPrefer_Self\n",
    "## 13th: Uniform_PA * Uniform_Latest * Similar_Field * Prefer_Self\n",
    "## 14th: Uniform_PA * Uniform_Latest * Similar_Field * NotPrefer_Self\n",
    "## 15th: Uniform_PA * Uniform_Latest * Different_Field * Prefer_Self\n",
    "## 16th: Uniform_PA * Uniform_Latest * Different_Field * NotPrefer_Self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9f4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "oldest = 1979\n",
    "superbeta = beta(a=10,b=1)\n",
    "superbeta_dist = np.array([superbeta.pdf((intime-oldest)/(outtime-oldest)) for outtime in range(2000,2019) for intime in range(1980,2018)]).reshape(2019-2000,2018-1980)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a8501e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 38)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superbeta_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea209ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "superbeta = beta(a=10,b=1)\n",
    "superbeta_dist = np.array([superbeta.pdf((intime-oldest)/(outtime-oldest)) for outtime in range(2000,2019) for intime in range(1980,2018)]).reshape(2019-2000,2018-1980)\n",
    "\n",
    "def cal_cite_edgeprobs(outcontent):\n",
    "    \n",
    "    outyear, outfield, outcitations = outcontent.year, content_fields[outcontent.id], outcontent.outcitations\n",
    "    outcocontents = set()\n",
    "    for author,_,_ in outcontent.authors:\n",
    "        for year in author_contents[author].keys():\n",
    "            if year < outyear:\n",
    "                for outcocontent in author_contents[author][year]:\n",
    "                    outcocontents.add(outcocontent)\n",
    "\n",
    "    edgeprobs = []\n",
    "    for incontent in outcitations:\n",
    "         \n",
    "        pnormal_pa = 0.1/content_cumcounts[outyear-1]+0.9*content_eachdgs[incontent][outyear-1]/content_sumdgs[outyear-1]\n",
    "        puniform_pa = 1/content_cumcounts[outyear-1]\n",
    "\n",
    "        pnormal_latest = superbeta_dist[outyear-2000, content_year[incontent]-1980]\n",
    "        puniform_latest = 1/(outyear-oldest)\n",
    "\n",
    "        psim_field = 1-(1-math.exp(-np.linalg.norm(outfield-content_fields[incontent],2)))/(1-math.exp(-2))\n",
    "        pdif_field = 1-psim_field\n",
    "        \n",
    "        ppre_self = 0.9/len(outcocontents) if incontent in outcocontents else 0.1/(content_cumcounts[outyear-1]-len(outcocontents))\n",
    "        pnot_self = 0.1/len(outcocontents) if incontent in outcocontents else 0.9/(content_cumcounts[outyear-1]-len(outcocontents))\n",
    "        \n",
    "        temp1 = np.outer([pnormal_pa,puniform_pa],[pnormal_latest,puniform_latest]).flatten()\n",
    "        temp2 = np.outer([psim_field,pdif_field],[ppre_self,pnot_self]).flatten()\n",
    "        edgeprobs.append(np.outer(temp1,temp2).flatten())\n",
    "    \n",
    "    return np.array(edgeprobs, dtype=np.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
