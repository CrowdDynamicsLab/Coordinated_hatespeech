{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4ca75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/yian3/.conda/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/u/yian3/.conda/envs/py39/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/u/yian3/.conda/envs/py39/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pklb\n",
    "import datetime\n",
    "from scipy.stats import beta\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import os\n",
    "import ast\n",
    "#from Content import *\n",
    "#from Venue import *\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590c2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "## People\n",
    "## Time\n",
    "## Topic\n",
    "## Media Contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97211ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../data/aliceysu'\n",
    "journalist = 'aliceysu'\n",
    "with open(os.path.join(out_dir, f'{journalist}_dict.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(out_dir, f'{journalist}_ids.pkl'), 'rb') as f:\n",
    "    map_id = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(out_dir, f'{journalist}_lan.pkl'), 'rb') as f:\n",
    "    map_lan = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(out_dir, f'{journalist}_type.pkl'), 'rb') as f:\n",
    "    map_type = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(out_dir, f'{journalist}_reply.pkl'), 'rb') as f:\n",
    "    map_reply = pickle.load(f)\n",
    "\n",
    "alice = pd.DataFrame.from_dict(data)\n",
    "alice_sort = alice.sort_values(by=['created_at'])\n",
    "conv = pd.read_csv(os.path.join(out_dir, f'{journalist}_conv_labels.csv'))\n",
    "\n",
    "data = pkl.load(open(os.path.join(out_dir, f'{journalist}_dict.pkl'), 'rb'))\n",
    "journal = pd.DataFrame.from_dict(data)\n",
    "journal_sort = pd.read_csv(os.path.join(out_dir, f'{journalist}_conv_labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c1ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_gap = []\n",
    "for item in journal_sort['time gap']:\n",
    "    if item < 240:\n",
    "        short_gap.append(item)\n",
    "        \n",
    "max_gap = 240\n",
    "data = []\n",
    "# for index, item in journal_sort.iterrows():\n",
    "#     data.append(superbeta.pdf(1-item['time gap'] / max_gap) + 1e-20)\n",
    "for item in short_gap:\n",
    "    data.append(superbeta.pdf(1 - item / max_gap) + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bafe082",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(set(journal_sort['conversation_id']))\n",
    "batch_data = []\n",
    "target_data = []\n",
    "conv_data = []\n",
    "ref_data = []\n",
    "id_data = []\n",
    "for idx in ids:\n",
    "    convs = journal_sort[journal_sort['conversation_id'] == idx]\n",
    "    convs_batch = convs[[\"type\", \"possibly_sensitive\", \"lang\", \"reply_settings\",\n",
    "                     \"retweet_count\", \"reply_count\", \"like_count\", \"quote_count\", \"impression_count\",\n",
    "                     \"mentions\", \"urls\"]]\n",
    "    conv_data.append(list(convs['conversation_id']))\n",
    "    ref_data.append(list(convs['reference_id']))\n",
    "    id_data.append(list(convs['tweet_id']))\n",
    "    batch_data.append(convs_batch.values.tolist())\n",
    "    target_data.append(list(convs['labels']))\n",
    "    \n",
    "label_data = target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a6292f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "for item in conv['context']:\n",
    "    temp_s = ''\n",
    "    if item == '0':\n",
    "        contexts.append('Unified Twitter Taxonomy')\n",
    "\n",
    "    else:\n",
    "        for i in ast.literal_eval(item):\n",
    "            temp_s  += (i['domain']['name'] + ' ' + i['entity']['name'])\n",
    "        contexts.append(temp_s)\n",
    "    \n",
    "annotations = []\n",
    "for item in conv['annotations']:\n",
    "    temp_s = ''\n",
    "    if item == '0':\n",
    "        annotations.append('Others')\n",
    "\n",
    "    else:\n",
    "        for i in ast.literal_eval(item):\n",
    "            temp_s += i['type'] + ' ' + i['normalized_text']\n",
    "\n",
    "        annotations.append(temp_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3128dd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>type</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>lang</th>\n",
       "      <th>reply_settings</th>\n",
       "      <th>...</th>\n",
       "      <th>urls</th>\n",
       "      <th>labels</th>\n",
       "      <th>context</th>\n",
       "      <th>annotations</th>\n",
       "      <th>topics</th>\n",
       "      <th>ppls</th>\n",
       "      <th>topics_sim</th>\n",
       "      <th>ppls_sim</th>\n",
       "      <th>time gap</th>\n",
       "      <th>global</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>24709718</td>\n",
       "      <td>9947</td>\n",
       "      <td>339010742</td>\n",
       "      <td>9947</td>\n",
       "      <td>9948</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unified Twitter Taxonomy</td>\n",
       "      <td>Others</td>\n",
       "      <td>['Unified Twitter Taxonomy', 'Ongoing News Sto...</td>\n",
       "      <td>['Others', 'Others']</td>\n",
       "      <td>[[0.7854743]]</td>\n",
       "      <td>[[0.8374142]]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[0, 0, 0], [1, 0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24709718</td>\n",
       "      <td>10491</td>\n",
       "      <td>805773481329704960</td>\n",
       "      <td>10491</td>\n",
       "      <td>10492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Unified Twitter Taxonomy</td>\n",
       "      <td>Other covid</td>\n",
       "      <td>['Unified Twitter Taxonomy', 'Unified Twitter ...</td>\n",
       "      <td>['Others', 'Others']</td>\n",
       "      <td>[[1.]]</td>\n",
       "      <td>[[0.8374142]]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[0, 0, 0], [1, 0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24709718</td>\n",
       "      <td>10020</td>\n",
       "      <td>14411162</td>\n",
       "      <td>10020</td>\n",
       "      <td>10021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unified Twitter Taxonomy</td>\n",
       "      <td>Others</td>\n",
       "      <td>['Unified Twitter Taxonomy', 'Unified Twitter ...</td>\n",
       "      <td>['Others', 'Others']</td>\n",
       "      <td>[[1.]]</td>\n",
       "      <td>[[0.8374142]]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[0, 0, 0], [1, 0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>964054011413483520</td>\n",
       "      <td>5887</td>\n",
       "      <td>3997957768</td>\n",
       "      <td>5894</td>\n",
       "      <td>5895</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Place Israel</td>\n",
       "      <td>Others</td>\n",
       "      <td>['Unified Twitter Taxonomy', 'Place Israel']</td>\n",
       "      <td>[\"Unified Twitter Taxonomy NewsUnified Twitter...</td>\n",
       "      <td>[[0.86925256]]</td>\n",
       "      <td>[[0.94740593]]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[0, 0, 0], [1, 4, 0], [2, 0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>990177925134630912</td>\n",
       "      <td>5887</td>\n",
       "      <td>3997957768</td>\n",
       "      <td>5892</td>\n",
       "      <td>5893</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Place Israel</td>\n",
       "      <td>Others</td>\n",
       "      <td>['Unified Twitter Taxonomy', 'Brand The Washin...</td>\n",
       "      <td>[\"Unified Twitter Taxonomy NewsUnified Twitter...</td>\n",
       "      <td>[[0.8028154]]</td>\n",
       "      <td>[[0.94740593]]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0, 0, 0], [1, 4, 1], [2, 0, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10627</th>\n",
       "      <td>10627</td>\n",
       "      <td>24709718</td>\n",
       "      <td>10301</td>\n",
       "      <td>1598727471712456704</td>\n",
       "      <td>10301</td>\n",
       "      <td>10302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Unified Twitter Taxonomy</td>\n",
       "      <td>Others</td>\n",
       "      <td>['Unified Twitter Taxonomy', 'Unified Twitter ...</td>\n",
       "      <td>['Others', 'Others']</td>\n",
       "      <td>[[1.]]</td>\n",
       "      <td>[[0.8374142]]</td>\n",
       "      <td>528.0</td>\n",
       "      <td>[[0, 0, 0], [1, 25, 25]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10628</th>\n",
       "      <td>10628</td>\n",
       "      <td>24709718</td>\n",
       "      <td>6208</td>\n",
       "      <td>1598727471712456704</td>\n",
       "      <td>6208</td>\n",
       "      <td>6210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Unified Twitter Taxonomy</td>\n",
       "      <td>Other Ultraman</td>\n",
       "      <td>['Unified Twitter Taxonomy', 'Unified Twitter ...</td>\n",
       "      <td>['Others', 'Others']</td>\n",
       "      <td>[[1.]]</td>\n",
       "      <td>[[0.8374142]]</td>\n",
       "      <td>105.0</td>\n",
       "      <td>[[0, 0, 0], [1, 7, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10629</th>\n",
       "      <td>10629</td>\n",
       "      <td>24709718</td>\n",
       "      <td>6208</td>\n",
       "      <td>1519143967886954496</td>\n",
       "      <td>6208</td>\n",
       "      <td>6209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Unified Twitter Taxonomy</td>\n",
       "      <td>Other UltramanOther UltramanPerson Xi Jinping</td>\n",
       "      <td>['Unified Twitter Taxonomy', 'Unified Twitter ...</td>\n",
       "      <td>['Others', 'Others']</td>\n",
       "      <td>[[1.]]</td>\n",
       "      <td>[[0.8374142]]</td>\n",
       "      <td>55.0</td>\n",
       "      <td>[[0, 0, 0], [1, 7, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10630</th>\n",
       "      <td>10630</td>\n",
       "      <td>24709718</td>\n",
       "      <td>0</td>\n",
       "      <td>1519143967886954496</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Unified Twitter Taxonomy</td>\n",
       "      <td>Organization Jiangsu Consumer Protection Commi...</td>\n",
       "      <td>['Unified Twitter Taxonomy', 'Unified Twitter ...</td>\n",
       "      <td>['Others', 'Others']</td>\n",
       "      <td>[[1.]]</td>\n",
       "      <td>[[0.8374142]]</td>\n",
       "      <td>298.0</td>\n",
       "      <td>[[0, 0, 0], [1, 2, 2]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10631</th>\n",
       "      <td>10631</td>\n",
       "      <td>24709718</td>\n",
       "      <td>671</td>\n",
       "      <td>1495347311110672385</td>\n",
       "      <td>671</td>\n",
       "      <td>672</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unified Twitter Taxonomy</td>\n",
       "      <td>Other Alice</td>\n",
       "      <td>['Unified Twitter Taxonomy', 'Unified Twitter ...</td>\n",
       "      <td>['Others', 'Others']</td>\n",
       "      <td>[[1.]]</td>\n",
       "      <td>[[0.8374142]]</td>\n",
       "      <td>674.0</td>\n",
       "      <td>[[0, 0, 0], [1, 2206, 2206]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10632 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0           author_id  conversation_id              user_id  \\\n",
       "0               0            24709718             9947            339010742   \n",
       "1               1            24709718            10491   805773481329704960   \n",
       "2               2            24709718            10020             14411162   \n",
       "3               3  964054011413483520             5887           3997957768   \n",
       "4               4  990177925134630912             5887           3997957768   \n",
       "...           ...                 ...              ...                  ...   \n",
       "10627       10627            24709718            10301  1598727471712456704   \n",
       "10628       10628            24709718             6208  1598727471712456704   \n",
       "10629       10629            24709718             6208  1519143967886954496   \n",
       "10630       10630            24709718                0  1519143967886954496   \n",
       "10631       10631            24709718              671  1495347311110672385   \n",
       "\n",
       "       reference_id  tweet_id  type  possibly_sensitive  lang  reply_settings  \\\n",
       "0              9947      9948     0                   0     1               0   \n",
       "1             10491     10492     0                   0     1               0   \n",
       "2             10020     10021     0                   0     1               0   \n",
       "3              5894      5895     0                   0     1               0   \n",
       "4              5892      5893     0                   0     1               0   \n",
       "...             ...       ...   ...                 ...   ...             ...   \n",
       "10627         10301     10302     0                   0     0               0   \n",
       "10628          6208      6210     0                   0     0               0   \n",
       "10629          6208      6209     0                   0     0               0   \n",
       "10630             0         1     0                   0     0               0   \n",
       "10631           671       672     0                   0    14               0   \n",
       "\n",
       "       ... urls  labels                   context  \\\n",
       "0      ...    0       1  Unified Twitter Taxonomy   \n",
       "1      ...    0       0  Unified Twitter Taxonomy   \n",
       "2      ...    0       1  Unified Twitter Taxonomy   \n",
       "3      ...    0       1              Place Israel   \n",
       "4      ...    0       1              Place Israel   \n",
       "...    ...  ...     ...                       ...   \n",
       "10627  ...    1       1  Unified Twitter Taxonomy   \n",
       "10628  ...    1       1  Unified Twitter Taxonomy   \n",
       "10629  ...    1       0  Unified Twitter Taxonomy   \n",
       "10630  ...    1       1  Unified Twitter Taxonomy   \n",
       "10631  ...    0       1  Unified Twitter Taxonomy   \n",
       "\n",
       "                                             annotations  \\\n",
       "0                                                 Others   \n",
       "1                                            Other covid   \n",
       "2                                                 Others   \n",
       "3                                                 Others   \n",
       "4                                                 Others   \n",
       "...                                                  ...   \n",
       "10627                                             Others   \n",
       "10628                                     Other Ultraman   \n",
       "10629      Other UltramanOther UltramanPerson Xi Jinping   \n",
       "10630  Organization Jiangsu Consumer Protection Commi...   \n",
       "10631                                        Other Alice   \n",
       "\n",
       "                                                  topics  \\\n",
       "0      ['Unified Twitter Taxonomy', 'Ongoing News Sto...   \n",
       "1      ['Unified Twitter Taxonomy', 'Unified Twitter ...   \n",
       "2      ['Unified Twitter Taxonomy', 'Unified Twitter ...   \n",
       "3           ['Unified Twitter Taxonomy', 'Place Israel']   \n",
       "4      ['Unified Twitter Taxonomy', 'Brand The Washin...   \n",
       "...                                                  ...   \n",
       "10627  ['Unified Twitter Taxonomy', 'Unified Twitter ...   \n",
       "10628  ['Unified Twitter Taxonomy', 'Unified Twitter ...   \n",
       "10629  ['Unified Twitter Taxonomy', 'Unified Twitter ...   \n",
       "10630  ['Unified Twitter Taxonomy', 'Unified Twitter ...   \n",
       "10631  ['Unified Twitter Taxonomy', 'Unified Twitter ...   \n",
       "\n",
       "                                                    ppls      topics_sim  \\\n",
       "0                                   ['Others', 'Others']   [[0.7854743]]   \n",
       "1                                   ['Others', 'Others']          [[1.]]   \n",
       "2                                   ['Others', 'Others']          [[1.]]   \n",
       "3      [\"Unified Twitter Taxonomy NewsUnified Twitter...  [[0.86925256]]   \n",
       "4      [\"Unified Twitter Taxonomy NewsUnified Twitter...   [[0.8028154]]   \n",
       "...                                                  ...             ...   \n",
       "10627                               ['Others', 'Others']          [[1.]]   \n",
       "10628                               ['Others', 'Others']          [[1.]]   \n",
       "10629                               ['Others', 'Others']          [[1.]]   \n",
       "10630                               ['Others', 'Others']          [[1.]]   \n",
       "10631                               ['Others', 'Others']          [[1.]]   \n",
       "\n",
       "             ppls_sim  time gap                             global  \n",
       "0       [[0.8374142]]       1.0             [[0, 0, 0], [1, 0, 0]]  \n",
       "1       [[0.8374142]]       1.0             [[0, 0, 0], [1, 0, 0]]  \n",
       "2       [[0.8374142]]       1.0             [[0, 0, 0], [1, 0, 0]]  \n",
       "3      [[0.94740593]]       1.0  [[0, 0, 0], [1, 4, 0], [2, 0, 0]]  \n",
       "4      [[0.94740593]]       0.0  [[0, 0, 0], [1, 4, 1], [2, 0, 0]]  \n",
       "...               ...       ...                                ...  \n",
       "10627   [[0.8374142]]     528.0           [[0, 0, 0], [1, 25, 25]]  \n",
       "10628   [[0.8374142]]     105.0             [[0, 0, 0], [1, 7, 6]]  \n",
       "10629   [[0.8374142]]      55.0             [[0, 0, 0], [1, 7, 7]]  \n",
       "10630   [[0.8374142]]     298.0             [[0, 0, 0], [1, 2, 2]]  \n",
       "10631   [[0.8374142]]     674.0       [[0, 0, 0], [1, 2206, 2206]]  \n",
       "\n",
       "[10632 rows x 27 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "ab438d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal['context'] = contexts\n",
    "journal['annotations'] = annotations\n",
    "journal_sort = journal.sort_values(by=['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8f424c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topics\n",
    "## people\n",
    "ref_ids = []\n",
    "\n",
    "journal_sort['topics'] = None\n",
    "journal_sort['ppls'] = None\n",
    "conv_ids = set(journal_sort['conversation_id'])\n",
    "for conv in list(conv_ids):\n",
    "    test_d = journal_sort[journal_sort['conversation_id'] == conv]\n",
    "    for index, item in test_d.iterrows():\n",
    "        i = 0\n",
    "        tweet_id = item['tweet_id']\n",
    "        ref_id = item['reference_id']\n",
    "        conv_id = item['conversation_id']\n",
    "        topic1 = item['context']\n",
    "        anno1 = item['annotations']\n",
    "        if len(test_d) == 1:\n",
    "            topic2 = 'Unified Twitter Taxonomy'\n",
    "            anno2 = 'Others'\n",
    "            journal_sort.at[index, 'topics']=[text1, text2]\n",
    "            journal_sort.at[index, 'ppls']=[anno1, anno2]\n",
    "            continue\n",
    "        if ref_id not in ref_ids:\n",
    "            if tweet_id == test_d.iloc[0]['tweet_id']:\n",
    "                text2 = test_d[test_d['tweet_id']==test_d.iloc[1]['tweet_id']]['context'].item()\n",
    "                anno2 = test_d[test_d['tweet_id']==test_d.iloc[1]['tweet_id']]['annotations'].item()\n",
    "            else:\n",
    "                text2 = test_d.iloc[i-1]['context']\n",
    "                anno2 = test_d.iloc[i-1]['annotations']\n",
    "        else:\n",
    "            #print(tweet_id,ref_id)\n",
    "            text2 = test_d[test_d['tweet_id']==ref_id]['context'].item()\n",
    "            anno2 = test_d[test_d['tweet_id']==ref_id]['annotations'].item()\n",
    "\n",
    "        ref_ids.append(tweet_id)\n",
    "        \n",
    "        journal_sort.at[index, 'topics']=[text1, text2]\n",
    "        journal_sort.at[index, 'ppls']=[anno1, anno2]\n",
    "        #print(text2, journal_sort.at[index, 'topics'])\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f634ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_sort.to_csv(os.path.join(out_dir, f'{journalist}_context.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "692dbdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "10632it [17:00, 10.41it/s]\n"
     ]
    }
   ],
   "source": [
    "context_sims = []\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "for index, item in tqdm(journal_sort.iterrows()):\n",
    "    # Tokenize input text and convert to tensor\n",
    "    c1 = item['topics'][0]\n",
    "    c2 = item['topics'][1]\n",
    "    \n",
    "    a1 = item['ppls'][0]\n",
    "    a2 = item['ppls'][1]\n",
    "    inputs1 = tokenizer(c1, return_tensors=\"pt\")\n",
    "    inputs2 = tokenizer(c2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)\n",
    "        outputs2 = model(**inputs2)\n",
    "\n",
    "    # The last hidden state is the sequence of hidden states of the last layer of the model\n",
    "    last_hidden_states1 = outputs1.last_hidden_state\n",
    "    last_hidden_states2 = outputs2.last_hidden_state\n",
    "\n",
    "    # Optionally, use the [CLS] token's embedding as the representation for the entire sentence\n",
    "    sentence_embedding1 = last_hidden_states1[:, 0, :]\n",
    "    sentence_embedding2 = last_hidden_states2[:, 0, :]\n",
    "\n",
    "    cosine = np.dot(sentence_embedding1,sentence_embedding2.T)/(norm(sentence_embedding1)*norm(sentence_embedding2))\n",
    "    context_sims.append(cosine)\n",
    "    #print(sentence_embedding.shape)\n",
    "    \n",
    "journal_sort['topics_sim'] = context_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "da53ee76",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "10632it [13:57, 12.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "anno_sims = []\n",
    "for index, item in tqdm(journal_sort.iterrows()):\n",
    "    # Tokenize input text and convert to tensor\n",
    "    c1 = item['topics'][0]\n",
    "    c2 = item['topics'][1]\n",
    "    \n",
    "    a1 = item['ppls'][0]\n",
    "    a2 = item['ppls'][1]\n",
    "    inputs1 = tokenizer(a1, return_tensors=\"pt\")\n",
    "    inputs2 = tokenizer(a2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)\n",
    "        outputs2 = model(**inputs2)\n",
    "\n",
    "    # The last hidden state is the sequence of hidden states of the last layer of the model\n",
    "    last_hidden_states1 = outputs1.last_hidden_state\n",
    "    last_hidden_states2 = outputs2.last_hidden_state\n",
    "\n",
    "    # Optionally, use the [CLS] token's embedding as the representation for the entire sentence\n",
    "    sentence_embedding1 = last_hidden_states1[:, 0, :]\n",
    "    sentence_embedding2 = last_hidden_states2[:, 0, :]\n",
    "\n",
    "    cosine = np.dot(sentence_embedding1,sentence_embedding2.T)/(norm(sentence_embedding1)*norm(sentence_embedding2))\n",
    "    anno_sims.append(cosine)\n",
    "    #print(sentence_embedding.shape)\n",
    "    \n",
    "journal_sort['ppls_sim'] = anno_sims\n",
    "journal_sort = pd.read_csv(os.path.join(out_dir, f'{journalist}_context.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "6033f78f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "034022e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_time = []\n",
    "test_d = journal_sort[journal_sort['conversation_id']==4]\n",
    "for index, item in test_d.iterrows():\n",
    "    anchor_time.append(datetime.strptime(item['created_at'][:19], date_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3c17c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = anchor_time[1] - anchor_time[0]\n",
    "anchor_hours = divmod(anchor.total_seconds() , 3600)[0]  # 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "941a8f68",
   "metadata": {},
   "source": [
    "date_format = \"%Y-%m-%dT%H:%M:%S\" \n",
    "datetime.strptime(data[0]['created_at'][:19], date_format)\n",
    "journal_sort.at[0, 'created_at'] - journal_sort.at[4, 'created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e097cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## time\n",
    "from datetime import datetime\n",
    "ref_ids = []\n",
    "date_format = \"%Y-%m-%dT%H:%M:%S\" \n",
    "journal_sort['time gap'] = None\n",
    "conv_ids = set(journal_sort['conversation_id'])\n",
    "for conv in list(conv_ids):\n",
    "    test_d = journal_sort[journal_sort['conversation_id'] == conv]\n",
    "    time0 = datetime.strptime(test_d.iloc[0]['created_at'][:19], date_format)\n",
    "    k = 0\n",
    "    for index, item in test_d.iterrows():\n",
    "        tweet_id = item['tweet_id']\n",
    "        ref_id = item['reference_id']\n",
    "        conv_id = item['conversation_id']\n",
    "        time1 = datetime.strptime(item['created_at'][:19], date_format)\n",
    "        if len(test_d) == 1:\n",
    "            journal_sort.at[index, 'time gap']= float(anchor_hours) #/120\n",
    "            continue\n",
    "        if ref_id not in ref_ids:\n",
    "            if tweet_id == test_d.iloc[0]['tweet_id']:\n",
    "                time2 = datetime.strptime(item['created_at'][:19], date_format) - anchor\n",
    "            else:\n",
    "                time2 = datetime.strptime(test_d.iloc[k-1]['created_at'][:19], date_format)\n",
    "        else:\n",
    "            #print(test_d[test_d['tweet_id']==ref_id]['created_at'][:19].item())\n",
    "            time2 = datetime.strptime(test_d[test_d['tweet_id']==ref_id]['created_at'].item()[:19], date_format)\n",
    "    \n",
    "\n",
    "        ref_ids.append(tweet_id)\n",
    "        gap_in_s = (time1 - time2).total_seconds() \n",
    "        journal_sort.at[index, 'time gap']= float(divmod(gap_in_s, 3600)[0]) #/ 120\n",
    "        \n",
    "        k += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867368df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PA * Latest * Field * Self = 2*2*2*2 = 16\n",
    "## PA: Normal PA (p=0.1, p/#nodes + (1-p)indegree/sum_of_indegree), Uniform (1/#nodes)\n",
    "## Latest: Normal Latest (beta(10,1), x=1-(outyear-inyear)/(outyear-oldest_year)), Uniform (1/(outyear-oldest_year))\n",
    "## Field: Similar (1-(1-e^(-||x-y||_2))/(1-e^(-2)), x&y L2-normalized), Different ((1-e^(-||x-y||_2))/(1-e^(-2)), x&y L2-normalized)\n",
    "## Self: Prefer (coauthors: 0.9/#coauthors, non-coauthors: 0.1/#non-coauthors), Not Prefer (coauthors: 0.1/#coauthors, non-coauthors: 0.9/#non-coauthors)\n",
    "\n",
    "## 1st: Normal_PA * Normal_Latest * Similar_Field * Prefer_Self\n",
    "## 2nd: Normal_PA * Normal_Latest * Similar_Field * NotPrefer_Self\n",
    "## 3rd: Normal_PA * Normal_Latest * Different_Field * Prefer_Self\n",
    "## 4th: Normal_PA * Normal_Latest * Different_Field * NotPrefer_Self\n",
    "## 5th: Normal_PA * Uniform_Latest * Similar_Field * Prefer_Self\n",
    "## 6th: Normal_PA * Uniform_Latest * Similar_Field * NotPrefer_Self\n",
    "## 7th: Normal_PA * Uniform_Latest * Different_Field * Prefer_Self\n",
    "## 8th: Normal_PA * Uniform_Latest * Different_Field * NotPrefer_Self\n",
    "## 9th: Uniform_PA * Normal_Latest * Similar_Field * Prefer_Self\n",
    "## 10th: Uniform_PA * Normal_Latest * Similar_Field * NotPrefer_Self\n",
    "## 11th: Uniform_PA * Normal_Latest * Different_Field * Prefer_Self\n",
    "## 12th: Uniform_PA * Normal_Latest * Different_Field * NotPrefer_Self\n",
    "## 13th: Uniform_PA * Uniform_Latest * Similar_Field * Prefer_Self\n",
    "## 14th: Uniform_PA * Uniform_Latest * Similar_Field * NotPrefer_Self\n",
    "## 15th: Uniform_PA * Uniform_Latest * Different_Field * Prefer_Self\n",
    "## 16th: Uniform_PA * Uniform_Latest * Different_Field * NotPrefer_Self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea209ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "superbeta = beta(a=10,b=1)\n",
    "\n",
    "# def cal_cite_edgeprobs(df):\n",
    "#     edge_prob = []\n",
    "#     for index, item in df.iterrows():\n",
    "         \n",
    "#         ppl_sim = float(item['ppls_sim'][2:-2])\n",
    "#         ppl_dif = 1 - ppl_sim\n",
    "\n",
    "#         topic_sim = float(item['topics_sim'][2:-2])\n",
    "#         topic_dif = 1 - topic_sim\n",
    "\n",
    "#         pnormal_latest = superbeta.pdf(1-item['time gap']) + 1e-20\n",
    "#         puniform_latest = 1 / 168\n",
    "\n",
    "#         temp1 = np.array([ppl_sim, ppl_dif])\n",
    "#         temp2 = np.outer([topic_sim,topic_dif],[pnormal_latest,puniform_latest]).flatten()\n",
    "#         result = np.outer(temp1,temp2).flatten()\n",
    "#         edge_prob.append(result)\n",
    "    \n",
    "#     return np.array(edge_prob, dtype=np.float32)\n",
    "def cal_cite_edgeprobs(ids):\n",
    "    edge_prob = [] #np.zeros((len(ids), 8))\n",
    "    for idx in ids:\n",
    "        convs = journal_sort[journal_sort['conversation_id'] == idx]\n",
    "        temp = []\n",
    "        max_gap = 240 # max(convs['time gap'])\n",
    "        for index, item in convs.iterrows():\n",
    "\n",
    "            ppl_sim = float(item['ppls_sim'][2:-2])\n",
    "            ppl_dif = 1 - ppl_sim\n",
    "\n",
    "            topic_sim = float(item['topics_sim'][2:-2]) - 1e-3\n",
    "            topic_dif = 1 - topic_sim\n",
    "\n",
    "            pnormal_latest = (superbeta.pdf(1-item['time gap'] / max_gap) + 1e-10) / 10\n",
    "            puniform_latest = 1 / 120\n",
    "\n",
    "            temp1 = np.array([ppl_sim, ppl_dif])\n",
    "            temp2 = np.outer([topic_sim,topic_dif],[pnormal_latest,puniform_latest]).flatten()\n",
    "            result = np.outer(temp1,temp2).flatten()\n",
    "            temp.append(result)\n",
    "        edge_prob.append(np.array(temp))\n",
    "    return edge_prob\n",
    "edgeprob = cal_cite_edgeprobs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0667dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, f'{journalist}_edgeprob.pkl'),'wb') as f:\n",
    "    pickle.dump(edgeprob, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51563449",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, f'{journalist}_edgeprob.pkl'),'rb') as f:\n",
    "    edgeprob = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0891cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
